Машинное обучение
 Подцепко Игорь
 27 июня 2023 г.
 1
Оглавление стр.2из95
 Оглавление
 Лекция0 Организационнаялекция 5
 1 Планкурса . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
 2 Системаоценки . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
 Лекция1 Настройкагиперпараметров 7
 3 Выборалгоритмаинастройкагиперпараметров . . . . . . . . . . . . . . . . . 7
 4 Конфигурациягиперпараметров . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
 5 Поискпосетке. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
 6 Случайныйпоиск. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
 7 Эволюционныеалгоритмы . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
 8 Реализацияметодовпонастройкегиперпараметров . . . . . . . . . . . . . . . 13
 Лекция2 Автоматическоедифференцирование 14
 9 Сведениезадачиобучениякзадачедифференцирования . . . . . . . . . . . . 14
 10Дифференцированиесоставныхфункцийпографувычислений . . . . . . . 15
 11Дифференцированиесложныхфункций . . . . . . . . . . . . . . . . . . . . . . 18
 12Улучшенияградиентногоспуска. . . . . . . . . . . . . . . . . . . . . . . . . . . 19
 13Методывторогопорядка . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
 14Дополнительныетемы. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
 Лекция3 Глубокоеобучение 24
 15Послойныеархитектуры. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
 16Функцииактивации . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
 17Декомпозициямоделей . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
 18ДропаутиБатчеваянормализация . . . . . . . . . . . . . . . . . . . . . . . . . 30
 19Инициализацияпараметров . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
 Лекция4 Свёрточныесетииработасизображениями 33
 20Особенностиизображений . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
 21Свёрточныесети . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
 22Обзорархитектур . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
 23Задачикомпьютерногозрения . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
 Лекция5 Фреймворкидляглубокогообучения 39
 24Краткийобзорпопулярныхфреймворков . . . . . . . . . . . . . . . . . . . . . 39
 25ПодробнееоPyTorch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
 Лекция6 Рекуррентныесетииработаспоследовательностями 42
 26Рекуррентныенейронныесети. . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
 27Обработкапоследовательностей . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
Оглавление стр.3из95
 28Продвинутыеархитектуры . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
 29Большесвязей. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
 30Механизмвнимания . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
 31Тансформеры . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
 Лекция7 Анализтекста 55
 32Определениетекстаипостроениеэмбеддингов . . . . . . . . . . . . . . . . . . 55
 33ПредобработкатекстасредствамиPython . . . . . . . . . . . . . . . . . . . . . . 56
 34Алгоритмыклассическогомашинногообучения . . . . . . . . . . . . . . . . . 56
 35Обучениеэмбеддинговслов . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
 Лекция8 Анализзвука 59
 36Короткоозвуке . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
 37Признаковоеописаниеаудио-сэмплов . . . . . . . . . . . . . . . . . . . . . . . 60
 Лекция9 Кластеризация 63
 38Базоваяпостановказадачикластеризации . . . . . . . . . . . . . . . . . . . . . 63
 39Оценкакачествакластеризации . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
 40Эвристическиеалгоритмыкластеризации . . . . . . . . . . . . . . . . . . . . . 65
 40.1 k-Means . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
 40.2MeanShift. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
 40.3DBSCAN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
 40.4 EMGMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
 40.5 Кластеризациянаосновеграфов . . . . . . . . . . . . . . . . . . . . . . . . 68
 40.6Иерархическаякластеризация . . . . . . . . . . . . . . . . . . . . . . . . . 68
 40.7Спектральнаякластеризация . . . . . . . . . . . . . . . . . . . . . . . . . . 69
 41Нейросетевыеметодыкластеризации. . . . . . . . . . . . . . . . . . . . . . . . 69
 41.1Сквознаяидвухэтапнаякластеризация. . . . . . . . . . . . . . . . . . . . 69
 Лекция10 Выборпризнаков 70
 42Уменьшениеразмерности. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
 43Встроенныеметоды . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
 44Методы-обертки. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
 45Фильтры . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
 46Гибридыиансамбли . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
 Лекция11 Извлечениепризнаков 79
 47Методглавныхкомпонент . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
 48Автокодировщики . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
 49t-SNE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
 50Обучениенаодномпримере . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
 51Переносзнаний . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
 52Самообучение . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
 53Векторныепредставленияслов. . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
 Лекция12 Генеративныемодели 89
 54Задачагенерацииновыхобъектов . . . . . . . . . . . . . . . . . . . . . . . . . . 89
 55PixelCNNиPixelRNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
 56Вариационныеавтокодировщики . . . . . . . . . . . . . . . . . . . . . . . . . . 90
Оглавление стр.4из95
 57Генеративно-состязательныемодели . . . . . . . . . . . . . . . . . . . . . . . . 92
 58ИнтересныеидеивGAN’ах . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
 59Диффузныемодели . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
Лекция 0. Организационная лекция
 стр. 5 из 95
 Лекция 0
 Организационная лекция
 Лектор: Алексей Сергеевич Забашта
 1 Планкурса
 1. Гиперпараметры
 • Настройка гиперпараметров
 2. Глубокое обучение
 • Автоматическое дифференцирование
 • Базовые архитектуры
 • Свёрточные и рекуррентные сети
 • Анализ не табличных данных
 3. Задачи обучения без учителя
 • Кластеризация
 • Выделение признаков
 • Генерация данных
 2 Системаоценки
 Для получения оценки есть следующие пути (не взаимозаменяемые):
 • Практика — сдача задач и лабораторных работ
 • Теория — написание контрольной или сдача экзамена
Лекция 0. Организационная лекция
 стр. 6 из 95
 • Бонусные баллы
 Финальная оценка вычисляется по следующей формуле:
 FinalScore = min
 √
 T ·P +B′,100
 где
 • T ∈[0;120] — баллы за теоретическую часть;
 • P ∈[0;120] — баллы за практическую часть;
 • B′ ∈[0;30) — скорректированные бонусные баллы, которые получаются из обыч
ных мягко ограничивающей функцией:
 B′ = 30B
 30 +B.
 Баллы за теорию можно получить либо на контрольной работе, на которой можно
 получить от 0 до 60 баллов, либо на экзамене:
 • Экзамен не суммируется с контрольной;
 • Вслучае попытки сдачи баллы за теорию будут равны баллу за экзамен;
 • Состоит из теормина (60 баллов) и ответов на вопросы билета (ещё 60 баллов).
 Баллы за практику можно получать за лабораторные работы и задачи. Первые нуж
но защищать на практиках и по ним есть мягкий дедлайн: за них можно получить
 K · 0,6+ 0,4
 1+w
 баллов, где w- это число недель после дедлайна. Задачи сдаются на
 https://codeforces.com/, их не нужно защищать и по ним строгий дедлайн. Распреде
ление баллов между лабораторными работами и задачами- 70/50 (суммарно можно
 набрать 120 баллов).
 Бонусные баллы можно получить за:
 • написание конспектов
 • написание визуализаторов
 • улучшение курса
Лекция 1. Настройка гиперпараметров
 стр. 7 из 95
 Лекция 1
 Настройка гиперпараметров
 Лектор: Алексей Сергеевич Забашта
 3 Выборалгоритмаинастройкагиперпараметров
 Рис. 1.1: Схема обучения алгоритма
 Определение. Различные определения гиперпараметров:
 • Параметры алгоритма обучения.
 • Параметры, которые не меняются во время обучения.
 • Параметры которые можно установить до наблюдения набора данных.
 • Структурные параметры.
Лекция 1. Настройка гиперпараметров
 стр. 8 из 95
 Определение. Задача настройки гиперпараметров строится следующим образом.
 Пусть:
 • D—наборданных
 • A—алгоритмобучения
 • p—гиперпараметры алгоритма обучения
 • L—функцияошибки(валидация)
 Требуется найти pbest = HyperParameterTuning(A,D,L) = argmin
 L(Ap,D)
 p
 Типичный вид алгоритма обучения гиперпараметров выглядит как перебор гиперпара
метровпонекоторомуалгоритму,проверкарезультатанаданныхитестированиемодели
 на валидационной части данных. Фактически это тоже алгоритм обучения, просто более
 сложный.
 Определение. Задача выбора алгоритма — для каждого алгоритма требуется выбрать
 наилучший для него алгоритм обучения. Данную проблему формализовал Rice J. R. в
 своей статье "The algorithm selection problem":
 Abest = AlgorithmSelection(D,L) = argmin
 A
 L(A,D)
 Примечание. Задачи настройки гиперпараметров и выбора алгоритма хорошо сводятся
 друг к другу, так как алгоритм можно считать гиперпараметром, а алгоритмом можно
 считать алгоритм с фиксированными параметрами. Однако второй подход, например,
 может приводить к очень сильному разрастанию числа алгоритмов.
 На практике требуется выполнять разделение задач поиска алгоритма и классификации,
 несмотря на то, что они сводятся друг к другу:
 • Гиперпараметры как и признаки можно типизировать числом и категорией.
 • Можно отнести категориальные гиперпараметры к "алгоритмама числовые- к
 "гиперпараметрам".
 Примечание. Может показаться логичной идея разделять "как естьоднако некоторые
 алгоритмы очень сильно изменяются при различных гиперпараметрах, например
 • Функция предсказания kNN с априорными весами w:
 
  
class(x′) = sign
 y · w(x) · κ ρ(x′,x)
 (x,y)∈Dtrain
 
 h
 
Лекция 1. Настройка гиперпараметров
 стр. 9 из 95
 • Функция предсказания SVM в общем виде:
 
 class(x′) = sign
  
(x,y)∈Dtrain
 • Функция предсказания SVM с линейным ядром:
 class(x′) = sign
 
 y · λ(x) · κ(x′,x)
 αj · x′
 j
 j
 
 Таким образом, алгоритм с одними гиперпараметрами может быть больше похож на
 другой алгоритм, чем на такой же алгоритм, но с другими гиперпараметрами.
 4 Конфигурациягиперпараметров
 Мыбудемрассматривать два типа конфигурации гиперпараметров:
 • линейную
 • древовидную
 На рис. 1.2 представлен пример конвейера, который подбирает гиперпараметры для
 моделей всех этапов обработки данных:
 Рис. 1.2: Pipeline преобразований
 Древовидная конфигурация (рис. 1.3) более естественная, так как перебираются парамет
ры, необходимые для конкретного алгоритма.
 На рис. 1.4 в общем виде представлена схема древовидной конфигурации.
Лекция 1. Настройка гиперпараметров
 стр. 10 из 95
 Рис. 1.3: Пример древовидной конфигурации
 Рис. 1.4: Общий вид древовидной конфигурации
 5 Поискпосетке
 Определение. Grid search (поиск по сетке) — алгоритм при котором перебираются
 все возможные комбинации значений (числовые гиперпараметры дискретизируются),
 которые образуют декартово произведение списков значений гиперпараметров (сетку).
 Примечание. Комбинации значений можно получать на лету, не тратя много памяти,
 однако поиск по сетке работает очень долго. Причем главная проблема здесь еще и в
 том, что время его работы трудно рассчитать, если оно зависит от гиперпараметров. Если
 после поиска время осталось, значит была взята слишком "крупная"сетка. Если времени
 не хватило, значит были рассмотрены не все комбинации, и такой промежуточный
 результат будет предвзят к порядку гиперпараметров.
 Примечание. Использованиедревовиднойконфигурацииможетзначительноуменьшить
Лекция 1. Настройка гиперпараметров
 стр. 11 из 95
 кол-во комбинаций, требующих перебора.
 В sklearn есть реализация GridSearchCV.
 6 Случайныйпоиск
 Определение. Random search (случайный поиск) — алгоритм, при котором значения
 гиперпараметров задаются случайными распределениями. На каждой итерации комби
нациязначенийгиперпараметроввыбираетсяслучайно.Числоитерацийограничивается
 лимитом вычислений.
 В том жеsklearn есть RandomizedSearchCV, реализующий алгоритм случайного поиска
 гиперпараметров.
 Анализ случайного поиска:
 • Можетработать с гиперпараметрами у которых бесконечное число значений.
 • Нетребуется использовать древовидную конфигурацию.
 • Прощерассчитать время работы, так как процесс оптимизации можно остановить
 в любой момент.
 • Плохоработает с категориальными гиперпараметрами, так как некоторые комбина
циимогутповторяться, а некоторыемогутбытьвообщенерассмотрены.Например,
 для получения всех n значений требуется в среднем O(nlogn) повторов.
 • Получается, что случайный поиск работает плохо там, где поиск по сетке работает
 хорошо.
 7 Эволюционныеалгоритмы
 Определение. Black-Box Optimization (оптимизация "черного ящика") — задача, при
 которой алгоритм оптимизации может вычислять функцию ошибки L на произвольном
 входе из её области определения, но не может получить никакой дополнительной ин
формации о данной функции. Как правило, применяется для не дифференцируемых, не
 выпуклых или медленно вычислимых функций.
 Определение. Критерии остановки и сравнения алгоритмов оптимизации:
 • МинимумLпрификсированном времени или числе вызовов L.
 • Количество времени или число вызовов L для достижения требуемого значения L.
 Для оптимизации абстрактных объектов помимо функции ошибки (приспособленности)
 L(x) иногда нужны операторы:
Лекция 1. Настройка гиперпараметров
 стр. 12 из 95
 1. Генерации- создает новый объект:
 rand() : X
 2. Мутации- делает из объекта x новый объект x′, который похож на x:
 mutate(x) : X →X
 3. Кроссовера (кроссинговера)- делает из двух объектив третий, который одновре
менно похож на первый и второй:
 cross(x1, x2) : X ×X → X
 Примеры абстрактный алгоритмов:
 1. Randomsearch (случайный поиск) использует только оператор случайной генера
ции. Является базовым (наивным) решением задачи оптимизации.
 2. Hill climbing (поиск с восхождением к вершине) делает случайные изменения (мута
ции) в надежде улучшить результат.
 3. Simulated annealing (алгоритм имитации отжига) похож на предыдущий, но
 иногда может переходить в более плохое состояние.
 4. Генетический алгоритм использует сразу несколько объектов (популяцию) для
 оптимизации. Помимо мутации скрещивает объекты кроссовером.
 Вместо абстрактного пространства рассматривается X = Rm, так как с числами можно
 производить больше операций.
 Примечание. Численные методы являются частным случаем абстрактных.
 Пример. Переход к абстрактной оптимизации
 rand() = (N(0,1),...,N(0,1))
 mutate(x) = x+(N(0,ϵ),...,N(0,ϵ))
 cross(x1, x2) = x1 + x2
 2
 Примечание. Вместо небольшого изменения каждой координаты, можно менять случай
ное подмножество координат.
 Функций ошибки может быть несколько, например, ошибка решения и время работы.
 Нельзя просто суммировать величины с разными размерностями, однако можно на
ходить оптимум по какой-нибудь функции ошибки при фиксированных значениях
 остальных. Результатом многокритериальной оптимизации является парето-фронт
 решений.
Лекция 1. Настройка гиперпараметров
 стр. 13 из 95
 Рис. 1.5: Парето-фронт
 Примечание. Для применения алгоритмов поиска в числовых пространствах требует
ся функция, которая будет отображать вектор чисел в комбинацию гиперпараметров.
 Например, если требуется сделать из чисел категорию, можно использовать стандарт
ное преобразование: onehot-encoding делает из категории число, а argmax- из числа
 категорию. Однако при модификации вектора чисел, если положение максимума не
 изменится, то в результате получится то же значение категории и значение функции
 ошибки не изменится. Чтобы не допускать такие проблемы лучше использовать сме
шанное пространство поиска:
 • Примутации категории её значение заменяется на случайное другое.
 • Прикроссовере двух категорий случайно выбирается одно из двух значений.
 • Примутации целого числа к нему прибавляется целочисленный сдвиг.
 • Прикроссовере двух целых чисел можно брать среднее и округлять.
 8 Реализацияметодов понастройке гиперпараметров
 Популярной библиотекой на языке Python для настройки гиперпараметров является
 библиотека optuna. Её можно использовать не только для настройки гиперпараметров.
 Для её использования достаточно реализовать функцию ошибки (objective), которая
 будет зависеть от текущей конфигурации (trial).
Лекция 2. Автоматическое дифференцирование
 стр. 14 из 95
 Лекция 2
 Автоматическое дифференцирование
 Лектор: Алексей Сергеевич Забашта
 9 Сведениезадачиобучениякзадачедифференциро
вания
 Задача машинного обучения состоит из трех частей:
 • Набор данных (выборка)
 • Модель
 • Функция ошибки
 Введем необходимую терминологию:
 1. Обучаемая функция: f(x1,x2,...,xn,a1,a2,...,am) : Rn+m → Rk, где n — число при
знаков, m — число обучаемых параметров, k — число предсказываемых (целевых)
 признаков.
 2. Набор данных: D = {(⃗xi,⃗yi)}.
 3. Функция ошибки: E(y1,...,y|D|,y1,...,y|D|), где yi — предсказанный вектор для i-го
 объекта, а yi — реальный. Например,
 MSE(y1,..., y|D|,y1,..., y|D|) =
 .
 1
 |D| · k
 |D|
 i=1
 k
 j=1
 (yi,j − yi,j)2
 4. Режим обучения: Etrain(a1,...,am) = E(f(⃗x1,⃗a),...,f(⃗x|D|,⃗a),y1,...,y|D|). Перемен
ные xзафиксированы функцией ошибки, а переменные a изменяются в процессе
 минимизации.
Лекция 2. Автоматическое дифференцирование
 стр. 15 из 95
 5. Режимпредсказания: fpredict(x1,...,xn) = f(x1,...,xn,a1,...am). Переменные x изме
няются от запроса к запросу, а переменные a зафиксированы в процессе обучения.
 Задачу машинного обучения можно свести к задаче оптимизации. Решать её далее
 можно градиентным спуском или эволюционным алгоритмом (однако, когда известен
 градиент, лучше использовать градиентный спуск).
 Примечание. Вычислять градиент можно автоматически.
 Примечание. Если функция не выпуклая, то можно:
 • Предположить, что функция ошибки почти выпуклая;
 • Обновлять параметры в градиентном спуске более хитрыми методами;
 • Попробовать на практике.
 10 Дифференцированиесоставныхфункцийпографу
 вычислений
 Определение. Составная функция — функция, которая состоит из элементарных
 функций, для которых мы умеем вычислять производную. Как правило, описывается
 графом вычислений.
 Определение. Сложная функция —функция, тип значения которой отличен от скаля
ра, может быть вектором, матрицей и так далее.
 Если собрать из нескольких функций одну, то её размер может экспоненциально расти —
 это наивное вычисление функции. Наш путь — это граф вычислений.
 Определение. Статический граф вычислений строится до вычисления функции.
 Можнозаранее выполнить какие-то оптимизации.
 Определение. Динамический граф вычислений строится во время вычисления функ
ции иявляется более гибким.
 Если выполнять вычисление производной по графувычисленийотпараметровкпослед
ней вершине, нам сначала потребуется для всех вершин в графе посчитать производную
 всех вершин по x1, затем по x2 и так далее. Это forward-подход. Однако можно считать
 производную f по всем вершинам графа, начиная с того, что df
 df 
= 1, и пересчитывая
 от конца к вершинам-параметрам. Тогда потребуется всего один проход по графу- это
 backward-подход.
 Примечание. На практике под термином forward понимают вычисление функции (так
 как forward-подход к вычислению производной бесполезен), а под backward- вычисле
ние производной.
Лекция 2. Автоматическое дифференцирование
 стр. 16 из 95
 Элементарный дифференцируемый блок (см. рисунок 2.6) может запоминать X и Y, а
 также любые другие промежуточные значения, необходимые при пересчёта производ
ной. Блоку для пересчёта производной целевой функции не нужно знать всю функцию
 целиком.
 Примечание. Тип и размер X совпадает с ∂E
 ∂X
 , а Y с ∂E
 ∂Y 
.
 Можносоставлять композиции блоков, чтобы получать новые блоки:
 Пересчет производной производится по цепному правилу и его обобщению:
 (f(g(x)))′ = g′(x) · f′(g(x)) =⇒ ∂f
 ∂x = ∂g
 ∂x
 ∂f(g1(x),g2(x),..., gn(x))
 ∂x
 =
 i
 ∂f
 ∂gi
 ∂f
 ∂g
 ∂gi
 ∂x
 Таким образом, можно сформулировать алгоритм дифференцирования по графу вычис
лений:
 1. Вычисляемсоставную функциюQ,сохраняяграфвычисленийG = (V,E) : (u,v) ∈
 E, если вершина (блок) v напрямую зависит от u;
Лекция 2. Автоматическое дифференцирование
 стр. 17 из 95
 2. Для всех вершин v устанавливаем:
 ∂Q
 ∂v =I[v =Q]
 3. Для каждого ребра (u,v) ∈ E в обратном порядке вычисления целевой функции:
 • Если v —этопростая функция, то
 ∂Q
 ∂u += ∂Q
 ∂v · ∂u
 ∂v
 • Если v —этосложная функция (вычисляется абстрактным блоком):
 ∂Q
 ∂u +=dfu→v
 ∂Q
 ∂v
 Примечание. Важно, что в данных правилах используется оператор +=, а не =. Это связа
но с тем, что (u,v) может быть не единственным ребром из вершины u, а по обобщению
 правила цепочки по всем таким ребрам производные необходимо суммировать.
 Примечание. В данном контексте dfu→v означает пересчет производной из ∂Q
 Пример. Дифференцирование простых функций:
 • Сумма:
 • Произведение:
 • Применение функции:
 y = xi=⇒dxi+=dy
 y = xi=⇒dxi+=dy·
 j=i
 y =f(x) =⇒dx +=dy·f′(x).
 Примечание. В данном контексте запись dx — это сокращение для ∂Q
 ∂v 
в ∂Q
 xj
 ∂u 
.
 ∂x 
, где Q — это конеч
ная вершина в графе. Таким образом, последнее правило можно переписать так:
 y =f(x) =⇒ ∂Q
 ∂x += ∂Q
 ∂y · ∂y
 ∂x = ∂Q
 ∂y ·f′(x).
Лекция 2. Автоматическое дифференцирование
 стр. 18 из 95
 Рис. 2.1: Пример дифференцирования функции F = sin(x+y)
 11 Дифференцированиесложныхфункций
 x·y
 До этого речь шла о скалярных функциях. Для сложных функций всё сложнее и обще
го алгоритма нет. Есть отдельный матричный подход. Однако, можно рассматривать
 частные случаи.
 Пример. Если сложное скалярное произведение можно посчитать следующей функцией:
 f or i ...
 f or j ...
 f or k ...
 i f (...)
 x = f(i , j , k)
 y = g(i , j , k)
 z = h(i , j , k)
 c [ z ] += a[x] * b[y]
 То её производная высчитывается такой:
 f or i ...
 f or j ...
 f or k ...
 i f (...)
 x = f(i , j , k)
 y = g(i , j , k)
 z = h(i , j , k)
 da[x] += dc[z] * b[y]
 db[y] += dc[x] * a[x]
 Пример. Другие матричные функции:
Лекция 2. Автоматическое дифференцирование
 стр. 19 из 95
 • Сумма
 • Произведение
 Y =
 Xi =⇒dXi +=dY,
 i
 Y =X1·X2 =⇒dX1 =dY ·XT
 2 , dX2 =XT
 1 ·dY,
 • Произведение Адамара (покомпонентное)
 Y =X1◦X2◦...◦Xn −→dXi =X1◦...◦Xi−1 ◦dY ◦Xi+1 ◦...◦Xn,
 • Функция (покомпонентное применение)
 Y =f(X),Yi,j = f(Xi,j) =⇒ dX = f′(X)◦dY,
 • Обратная матрица
 Y =X−1 =⇒dX =−X−T ×dY ×X−T =−YT ×dY ×YT,
 • Транспонирование матрицы:
 Y =XT =⇒dX =(dY)T,
 • Перестановка π элементов массива X:
 Y =πX =⇒dX =π−1dY,
 где π−1 — это обратная перестановка,
 • Сортировка массива: arg-sort зависит от массива X, но производная через arg-sort
 не пересчитывается. Поэтому производная "нечестная": использует π — информа
циюотом, какую перестановку выполнил arg-sort.
 12 Улучшенияградиентногоспуска
 Проблемыградиентного спуска:
 1. Градиент указывает направление наискорейшего убывания функции, он не указывает
 на минимум.
 2. Он указываем направление, но ничего не говорит о том, сколько надо двигаться,
 чтобы достичь хотя бы локального минимума.
 3. Градиентный спуск плохо работает с не выпуклыми функциями.
Лекция 2. Автоматическое дифференцирование
 стр. 20 из 95
 Определение. Обыкновенный градиентный спуск. w(0) — начальные значения, µ —
 learning rate:
 w(k+1) = w(k) − µ∂Lw(k)
 ∂w .
 Определение. Модификация Momentum(импульсный градиентный спуск). v — вектор
 изменений, γ — момент (обычно устанавливается 0,9):
 w(k+1) = w(k) − v(k),
 v(k+1) = γv(k) + µ∂Lw(k)
 ∂w
 Преимущества импульсного градиентного спуска:
 • Вцелом, быстрее на сложном "рельефе"при движении в правильном направлении;
 • Можетиспользоваться для обработки шумных градиентов;
 • Можетработать с очень маленькими градиентами.
 Недостатки:
 • Вносит дополнительную сложность в модель;
 • Может пропускать минимумы, если "разгонимся". Данный метод накапливает
 импульси,когдапроскакиваетминимум,гораздоменьшеразворачиваетсяобратно.
 Определение. Метод Нестерова (Nesterov accelerated gradient, NAG)- модификация гра
диентногоспуска,котораяпытаетсяисправитьуказанныйвышенедостатокимпульсного
 градиентного спуска и расчитывает градиент на один шаг вперед:
 w(k+1) = w(k) − v(k),
 v(k+1) = γv(k) + µ∂Lw(k) −v(k)
 ∂w
 Преимущества:
 • Вцелом, работает лучше на практике;
 • Сходимость доказана при определенных условиях.
 Определение. Метод Adagrad (adaptive gradient).
 gi,(k) = ∂L(w(k))
 w(k+1)
 i
 =w(k)
 i
 ∂wi
 − µ
 G(k)
 i,i + ε
 gi,(k)
Лекция 2. Автоматическое дифференцирование
 стр. 21 из 95
 где G- диагональная матрица, в которой каждый элемент i,i — это сумма квадратов
 градиентов gi,(k) до шага k, то есть G(k)
 i,i = k
 j=1
 gi,(j), а ε — сглаживающая переменная,
 предотвращающая деления на ноль.
 Преимущества:
 • Устраняет необходимость вручную настраивать скорость обучения. Большинство
 реализаций используют значение по умолчанию 0,01 и оставляют его как есть.
 Недостатки:
 • Накопление квадратов градиентов в знаменателе приводит к тому, что в процессе
 обучения сумма продолжает расти. В конце концов алгоритм перестает чему-либо
 учиться.
 Определение. Метод RMSProp (root mean square propagation) — борется с недостатком
 метода Adagrad и берет не сумму предыдущих градиентов, а их скользящее экспоненци
альное среднее взвешенное.
 w(k+1)
 i
 =w(k)
 E(k)[g2
 i] = γE(k−1)[g2
 i] + (1 − γ)g2
 i,(k)
 µ
 E(k)[g2
 i] + εgi,(k)
 i
 −
 где ε — сглаживающая переменная, предотвращающая деление на ноль, γ устанавлива
ется равным 0,9.
 Определение. Метод Adadelta — модификация градиентного спуска, которая исполь
зует апроксимацию матрицы вторых производных.
 w(k+1) = w(k) − µL′′(w(k)) −1L′(w(k))
 w(k+1) = w(k) + δw(k)
 L′′(w(k)) −1 сложно оценить, поэтому предполагаем, что это диагональная матрица:
 L′′(w(k)) ≈ diag ∂L2(w(k))
 ∂w2
 i
 δw(k)
 i
 ≈ ∂L2(w(k))
 ∂w2
 i
 ∂L2(w(k))
 ∂w2
 i
 ≈
 −1 ∂L(w(k))
 ∂wi
 ∂L(w(k))
 ∂wi
 δw(k)
 i
 E(k)[g2
 i] = γE(k−1)[g2
 i] + (1 − γ)g2
 i,(k)
 RMS(k)[gi] =
 E(k)[g2
 i] + ε
Лекция2. Автоматическоедифференцирование стр.22из95
 RMS(k)[δwi]= E(k)[δw2
 i]+ε
 ∂L2(w(k))
 ∂w2
 i
 ≈
 ∂L(w(k))
 ∂wi
 δw(k)
 i
 ≈ g(k)
 i
 δw(k−1)
 i
 = RMS(k)[gi]
 RMS(k−1)[δwi]
 w(k+1)
 i =w(k)
 i −RMS(k−1)[δwi]
 RMS(k)[gi] g(k)
 i
 Примечание.МетодAdadeltaнетребуетустановкискоростиобучения,однаконапрактике
 еёдобавляютдляповышенияпроизводительности.
 Определение.МетодAdam(adaptivemomentestimation.
 m(k)=E(k)[gi]=γ1E(k−1)[gi]+(1−γ1)gi,(k)
 b(k)=E(k)[g2
 i]=γ2E(k−1)[g2
 i]+(1−γ2)g2
 i,(k)
 Хочется,чтобыонибылинесмещенными:
 E(m(k))=E(g(k)), E(b(k))=E(g2
 (k))
 Чтобыудовлетворитьданномутребованию,понадобитсяследующаяпоправка:
 
  
  
 m(k)= m(k)
 1−γk
 1
 b(k)= b(k)
 1−γ2
 2
 w(k+1)=w(k)− µ
 b(k)+ε
 m(k).
 13 Методывторогопорядка
 Определение.МетодНьютона-Рафсона.
 L(a,D)→min
 w∈Rp
 • Выборначальныхзначений:w(0)=(w(0)
 1 ,...,w(0)
 p )
 •Шагитерации:
 w(t+1)=w(t)−µt L′′(w(t)) −1L′(w(t)),
 гдеL′(w(t))—градиентLвw(t),L′′(w(t))—гессианLвw(t),µt—шаг(обычноµt=1).
 Заметим,чтонавычислениегессианатребуетсякубическаясложность(авычислять
 еговданномслучаетребуетсявкаждойточке).Чтобыизбежатьэтогоиспользуются
 квазиньютоновскиеметоды,использующиеприближеннуюоценкугессиана.
Лекция 2. Автоматическое дифференцирование
 стр. 23 из 95
 Определение. BFGS(алгоритмBroyden–Fletcher–Goldfarb– Shanno) — одинизнаиболее
 широко применяемых квазиньютоновских методов.
 1. Обратный гессиан аппроксимируется как:
 (L′′(w(t)))−1 ≈ C(k+1) = (I − ρkskyT
 k)C(k)(I − ρkyksT
 k) + ρksksT
 k,
 где I —единичнаяматрица,ρk = 1
 yT
 k sk 
, sk = w(k+1)−w(k), yk = ∇L(w(k+1))−∇L(w(k))).
 Определение. L-BFGS илиLM-BFGS—модификацияBFGS,требующаяменьшепамяти.
 Определение. L-BFGS-B — модификация L-BFGS с поддержкой простых ограничений.
 Определение. NGD (natural gradient descent) — метод оптимизации второго порядка,
 использующий матрицу информации Фишера для аппроксимации гессиана.
 Примечание. Матрица информации Фишера вычисляется как матрица ковариаций гра
диента функции потерь на тренировочном множестве данных. Вместо полной матрицы
 можно использовать блочно-диагональную матрицу.
 14 Дополнительныетемы
 Производную по входу можно использовать для:
 1. Состязательных атак (adversarial attack), когда объект "незаметно"(для челове
ческого глаза) модифицируется с помощью производной, а предсказанный класс
 объекта изменяется.
 2. Проверки того, чему обучилась или переобучилась сеть на промежуточных слоях
 или выходе. Идея применяется в DeepDream стилизации.
 3. Переноса стиля.
 4. В генеративно-состязательных сетях (GAN- Generative Adversarial Nets).
Лекция 3. Глубокое обучение
 стр. 24 из 95
 Лекция 3
 Глубокое обучение
 Лектор: Алексей Сергеевич Забашта
 Примечание. Современное глубокое обучение основано на автоматическом дифференци
ровании, о котором речь шла на прошлой лекции.
 15 Послойныеархитектуры
 Заметим, что геометрический смысл знака скалярного произведения заключается в том,
 что он показывает, с какой стороны от гиперплоскости расположена точка.
 Примечание. Даже простые логические функции могут не быть линейно разделимы (не
 путать с линейностью из дискретной математики). Пример логической функции, которая
 не является линейно разделимой — XOR).
 Для аппроксимации функций можно использовать сети из линейных преобразований:
 AND x1∧x2 =[x1+x2− 3
 2 
>0]
 OR x1∨x2 =[x1+x2− 1
 2 
>0]
 NOT
 ¬x =[−x+ 1
 2 
> 0]
 XORпредставляется как композиция, поэтому его также можно записать в таком виде:
 x1 ∨x2 = [(x1 ∨x2)−(x1 ∧x2)− 1
 2 > 0] = [x1 +x2 −2x1x2 − 1
 2 > 0]
 Идея построения композиций из таких преобразований приводит к идее построения
 многослойных сетей. Каждый слой в них выполняет три операции:
 1. Умножение на матрицу A
 2. Сдвиг вектором b
Лекция 3. Глубокое обучение
 стр. 25 из 95
 Рис. 3.1: Пример для XOR
 3. Применение функции активации σ
 Примечание. Функция активации требуется, чтобы все слои нельзя было свести к одному
 линейному преобразованию.
 Рис. 3.2: Матричный вид
 Одно из преимуществ такого подхода состоит в том, то очень легко контролировать
 число параметров модели и контролировать недообучение/переобучение. Помимо это
го в таком виде очень легко считать производную, используя метод автоматического
 дифференцирования. Более того, данный процесс можно выполнить параллельно на
 видеокарте.
 Такая архитектура имеет много названий:
 • Mutlilayer Neural Network (многослойная нейронная сеть)
 • Fully connected neural network (FCNN, полносвязная нейронная сеть)
 • Feedforward neural network (FNN, нейронная сеть с прямой связью)
 • MultiLayer Perceptron (MLP, многослойный перцептрон)
 • DeepNeural Network (DNN, глубокая нейронная сеть)
 • Artificial Neural Network (ANN, искусственная нейронная сеть)
Лекция 3. Глубокое обучение
 стр. 26 из 95
 Преимущества такой архитектуры заключаются в следующем:
 1. Для логических функций: любую логическую функцию можно представить в
 виде ДНФ и КНФ. Следовательно, требуется не более двух слоёв, но с возможно
 экспоненциальным числом вершин на промежуточном слое.
 2. Для числовых функций: Теорема Цыбенко или Универсальная теорема аппрок
симации (1989). Искусственная нейронная сеть с одним скрытым слоем может
 аппроксимировать любую непрерывную функцию многих переменных с любой
 точностью при условии достаточного числа вершин на скрытом слое.
 Примечание. В любом случае процесс больше похож на интерполяцию, чем на аппрокси
мацию. На практике используют больше двух слоев.
 Недостатки:
 1. Приперестановкивершинвместесвесамиихреберфункциянеменяется(проблема
 симметрии).
 2. Перестановкаможетбытьнепрерывной,следовательноужедлядвухслоевфункция
 содержит много минимумов, функция ошибки не будет выпуклой.
 3. Проблема затухания градиента (vanishing gradient problem): с каждым слоем
 градиент затухает. Чем дальше блок от выхода, тем хуже он обучается.
 4. Проблема "взрыва"(разрастания) градиента (exploding gradient problem): с каж
дымслоем градиент может неограниченно расти. Иногда возникает, если присут
ствует выброс. При обновлении вектор параметров может быть испорчен.
 Примечание. Чтобы уменьшить влияние того факта, что у функции с такой архитектурой
 много минимумов, можно инициализировать параметры случайными значениями.
 Например, из нормального или равномерного распределения.
 Примечание. Для борьбы с разрастанием/затуханием градиента можно использовать
 следующие подходы:
 1. Использовать специальные преобразования (например, LSTM) или функции акти
вации (ReLU).
 2. Использовать предобработку данных или специальную нормализацию параметров
 (например, Xavier или He).
 3. Выполнять подрезку градиента:
 • Глобальную: ||g|| > c ⇒ gnew = c· gold
 ||g||
 • Локальную: |gi| > c ⇒ gi = c·sign(gi)
 Примечание. Один из способов борьбы с разрастанием и затуханием градиента- до
бавлять "обходной путь"для данных, минуя функцию активации (используется в LSTM,
Лекция 3. Глубокое обучение
 стр. 27 из 95
 GoogLeNet, ResNet):
 1. f(x) = tanhx+ax —функцияактивации, предложенная в статье Лекуна.
 2. Пропуск слоев в ResNet.
 16 Функцииактивации
 Определение. Гиперболический тангенс — функция активации, аналогичная сигмо
иде, но с другим диапазоном выходных значений ([−1,+1]):
 a =tanh(x) = ex −e−x
 ex +e−x
 Градиент гиперболического тангенса относительно входа
 ∂a
 ∂x =1−tanh2x
 Преимущества:
 1. Более "сильные"градиенты, потому что данные сосредоточены вокруг 0, а не 0,5.
 2. Меньше предвзятости к выходам скрытого слоя, поскольку теперь выходные дан
ные могут быть как положительными, так и отрицательными (с большой вероят
ностью в конце будет нулевое среднее значение).
 Определение. ReLU — функция активации, популярная в компьютерном зрении и
 распознавании речи
 a =h(x) = max(0,x)
 Градиент ReLU относительно входа:
 ∂a
 ∂x = 1, еслиx>0
 0, иначе
 Преимущества:
 1. Гораздо быстрее вычисляется
 2. Меньше проблем с исчезающим или взрывающимся градиентом
 3. Прореживает значения (примерно половина обнуляется)
 4. Без насыщения
 Недостатки:
 1. Несимметричная функция
Лекция 3. Глубокое обучение
 стр. 28 из 95
 2. Формально не дифференцируется в 0
 3. Большой градиент во время тренировки может привести к "смерти"нейрона. Более
 высокая скорость обучения смягчает проблему.
 Определение. Softplus — гладкая аппроксимация ReLU:
 a =h(x) = ln(1+ex)
 Преимущества:
 • Дифференцируема в 0
 • Неподвержена "отмиранию"нейронов
 Недостатки:
 • Медленно вычисляется
 • Эмпирически было выяснено, что она не превосходит ReLU
 Определение. Шумный(noisy) ReLU:
 h(x) = max(0,x+ε), εN(0,σ(x)).
 Определение. ReLU с утечкой (leaky ReLU):
 h(x) = x
 Определение. Параметрический ReLU:
 если x > 0,
 0, 01x иначе
 h(x) = x еслиx>0,
 βx иначе
 17 Декомпозициямоделей
 Еще одно преимущество моделей глубокого обучения в возможности декомпозиции мо
делей.Например,возможновыполнениепереносазнаний:когдаунасужеестьобученная
 функция и специфическая задача с подходящим входом, мы можем переиспользовать
 часть уже обученной функции, причем не только её архитектуру, но и её параметры.
 Вначале обучения пересчёт производной через ещё не обученные (новые) преобразова
ния будет портить параметры уже обученных (старых) преобразований при обновлении.
 Поэтому параметры уже обученных преобразований замораживаются (не обновляются).
 После обучения новых преобразований можно разморозить параметры и дообучить
 старые преобразования.
 Можновыполнять прореживание ResNet архитектуры:
Лекция 3. Глубокое обучение
 стр. 29 из 95
 Название
 Функция
 Производная
 Логистическая (сигмоида)
 f(x) = σ(x) = 1
 1 +e−x
 Гиперболический тангенс
 f(x) = tanh(x) = ex −e−x
 ex +e−x
 f′(x) = f(x)(1 − f(x))
 f′(x) = 1−f(x)2
 ReLU (Leaky ReLU), 0 ≤ α ≤ 1 f(x) = max(αx,x) = αx, x < 0
 x,
 SoftPlus
 f(x) = ln(1 +ex)
 x ≥0 f(x)= α, x<0
 f′(x) =
 1, x≥0
 1
 1 +e−x
 Arctg
 f(x) = tg−1(x)
 f′(x) = 1
 1 +x2
 SoftSign
 f(x) = x
 1
 (1 +|x|)2
 1 +|x|
 f′(x) =
 Таблица 3.1: Краткая сводка по основным функциям активации
 • Каждое преобразование имеет "обходной путь".
 • Бесполезные преобразования будут возвращать близкие к нулю значения.
 • Следовательно, можно удалить преобразования которые возвращают меньшие
 значения.
 • Значения можно оценивать на отдельном (валидационном наборе данных).
 • После удаления преобразований архитектуру требуется дообучить.
 Послойные преобразования тоже можно прореживать (выкидывать ребра из графа):
 • Полносвязное преобразование можно представить двудольным графом.
 • Вэтомграфе можноудалять (прореживать) рёбра, которые соответствуют наиболее
 бесполезным параметрам.
 • Полезность рёбер можно определять по формуле
 Li = a2
 i
 [H−1]i,i
 ,
 где ai– значение i-го параметра, а H– гессиан (матрица вторых производных).
 • Если вторая производная вычисляется долго, то вместо нее можно использовать
 просто |ai|.
 Примечание. После прореживания полносвязных преобразований, также лучше запус
кать дообучение.
Лекция 3. Глубокое обучение
 стр. 30 из 95
 Рис. 3.3: Связь функций активации
 18 ДропаутиБатчеваянормализация
 Определение. Дропаут(dropout) — преобразование, при котором на итерации обучения
 обнуляются выходы нейронов с некоторой вероятностью (часто 0.5).
 Преимущества
 1. Обнуленные нейроны не участвуют в обучении, не внося вклад в ошибку.
 2. Для каждого выхода фактически строится новая сеть, но у всех этих сетей общие
 параметры.
 3. Уменьшается соадаптация нейронов, потому что они не могут рассчитывать на
 соседей.
 4. Обучается более робастное (выбросоустойчивое) представление.
 5. Без дропаута сети значительно переобучаются.
 6. Дропаут приблизительно вдвое увеличивает число операций до сходимости.
 Примечание. Дропаут не идентичен прореживанию, так как на каждой итерации выклю
чаются разные(случайные)нейроны,авконцеобученияонивсеостаютсявключенными
 (никакие ребра не вырезаются).
 Примечание. Так как на каждой итерации сеть рассчитывает, что нейронов, например, на
 30% меньше, в параметры могут быть на 30% больше. Когда все нейроны сети включены,
 может потребоваться сделать поправку.
 Примечание. Когдаменяютсяпараметрыслоя,такжеменяютсяраспределенияеговыхода.
Лекция 3. Глубокое обучение
 стр. 31 из 95
 Определение. Батчевая (пакетная) нормализация — преобразование, которое стара
ется поддерживать константу ковариации для каждого выходного слоя:
 xd = xd−E[xd]
 D[xd] + ϵ
 Пакетный слой пакетной нормализации:
 yd = γdxd +βd.
 Примечание. Такая нормализация называется батчевой, так как математическое ожида
ние и дисперсия оцениваются по пакету данных.
 Примечание. При применении пакетной нормализации дропаут становится бесполез
ным.
 19 Инициализацияпараметров
 Примечание. Параметры сдвига можно инициализировать нулями, однако матрицы
 преобразований нельзя инициализировать нулями или любыми другими одинаковыми
 значениями, так что принято использовать случайные значения для их инициализации.
 Предположим, что у нас есть функция активации f, линейная вблизи 0 (например, tanh):
 f(x) = x
 Рассмотрим нейрон y (n– число нейронов на предыдущем слое):
 n
 y =
 i=1
 wixi
 Для каждого слоя мы хотим, чтобы дисперсия оставалась одинаковой. Это поможет нам
 не допустить "взрыва"и затухания градиента. Вычислим дисперсию y, считая, что входы
 и параметры независимы и получены из симметричных равномерных или нормальных
 распределений:
 Dy =D
 n
 =
 n
 i=1
 Ew2
 i Ex2
 i =
 i=1
 n
 wixi =
 n
 i=1
 n
 D(wixi) =
 i=1
 E(w2
 ix2
 i) − E2(wixi) =
 (Ew2
 i − E2wi)(Ex2
 i − E2xi) =
 i=1
 n
 i=1
 n
 E(w2
 ix2
 i) =
 DwiDxi = n·Dwi ·Dxi
 i=1
 Чтобы дисперсия выхода Dy была равна дисперсии входа Dx, требуется выполнение
 следующего условия: Dwi = 1
 n . В целом аналогичный подход, который выполняет
Лекция 3. Глубокое обучение
 стр. 32 из 95
 некоторое сглаживание между слоями, предлагает брать Dwi = 1
 ¯n . Где ¯n– это среднее
 арифметическое между числом входных нейронов и выходных.
 Определение. Метод инициализации Завьера (Xavier) — метод, который стремиться
 упростить прохождение сигнала через слой во время вычисления функции и градиента
 для линейной вблизи нуля функции активации. При вычислении весов этот метод
 опирается на вероятностное распределение (равномерное или нормальное) с дисперсией,
 равной
 2
 nin +nout
 ,
 где nin и nout — это кол-во нейронов на предыдущем и последующем слоях соответствен
но.
 Для функции активации ReLU, которая не является линейной в области нуля подходит
 другой метод инициализации.
 Определение. Метод инициализации Ге (He) — вариация метода Завьера, больше
 подходящая для функции ReLU, компенсирующая тот факт, что эта функция равна нулю
 на половине области определения. Дисперсия распределения, из которого необходимо
 получать начальные значения параметров, полагается равной
 2
 nin 
или 2
 nout
 .
 Примечание. На практике для функции активации ReLU метод Ге показывает себя значи
тельно лучше, чем метод Завьера.
Лекция 4. Свёрточные сети и работа с изображениями
 стр. 33 из 95
 Лекция 4
 Свёрточные сети и работа с
 изображениями
 Лектор: Алексей Сергеевич Забашта
 20 Особенностиизображений
 Как правило, цветное изображение кодируется трехмерной матрицей (тензором) со
 следующими размерностями:
 • Ширина
 • Высота
 • Глубина (кодирует цвет)
 Матрицу можно превратить в вектор признаков наивным образом, линеаризовав её.
 Однако в таком случае потеряются важные инварианты изображения.
 Определение. Аугментация — это методика создания дополнительных данных из
 имеющихся данных. Некоторые методы аугментации изображений:
 • Горизонтальное/вертикальное отражение
 • Обрезка
 • Размытие
 • Изменение контраста
 • Изменение насыщенности каналов
 • Изменение гаммы изображения
 Идеи, на которые опирается глубокое обучение при обработке изображений:
Лекция 4. Свёрточные сети и работа с изображениями
 стр. 34 из 95
 1. Локальное восприятие: каждое преобразование действует на небольшую часть
 объекта (тензора). Используются ядра (фильтры), чтобы определять одномерные
 и двумерные структуры объектов. Например, захват всех соседних пикселей для
 изображения.
 2. Общиепараметры: использовать небольшие и одинаковые наборы ядер для всех
 объектов, это приводит к уменьшению количества настраиваемых параметров.
 3. Subsampling/pooling:использоватьуменьшениеразмерностиизображений,чтобы
 обеспечить инвариантность масштабирования.
 21 Свёрточныесети
 При решении задач обработки изображений методами глубокого обучения выполняется
 с помощьюиспользования сверток. Однако параметры этих сверток являются парамет
рами глубокой нейронной сети. В данном случае свертку можно понимать как мягкую
 проверку того, что изображено на изображении.
 Примечание. Заметим, что при использовании сверток изменяется размер изображения,
 поэтому используется дополнение границ изображения- padding.
 Рис. 4.1: Виды padding’ов: a) нулевой сдвиг, б) расширение границ,
 в) зеркальный сдвиг, г) циклический сдвиг
 Заметим, что для трехмерной свертки требуется четырехмерное ядро, которое будет
 иметь размерность Dout × Din × KW × KH, так как изначально изображение имеет,
 например, Din = 3 канала для каждого цвета, но после применения свертки оно может
 иметь Dout = 16 каналов.
 Примечание. Размер после свертки пересчитывается по следующему правилу:
 O = I−K+2P
 S
 где
 • O–размервыходного изображения
 +1,
Лекция 4. Свёрточные сети и работа с изображениями
 стр. 35 из 95
 Рис. 4.2: Пример 3D-свертки
 • I–размер входного изображения
 • K–размерядер, используемых в свёрточном слое
 • S–сдвиг (stride)
 • P–числоэлементов заполнения (padding)
 Определение. Пулинг(pooling)—свёрточноепреобразование,котороеиспользуетсядля
 снижения размерности и декорреляции нейронов. Для пулинга используются обычно
 простые функции, такие как максимум или сумма, а шаг S устанавливается достаточно
 большим (например, S = K). Формула для пересчета размера после пулинга:
 O = I−PS
 S +1,
 где
 • O–размервыходного изображения
 • I–размер входного изображения
 • S–сдвиг (stride)
 • PS–число элементов пулинга в каждой итерации
 22 Обзорархитектур
 Примеры архитектур свёрточных сетей:
Лекция 4. Свёрточные сети и работа с изображениями
 стр. 36 из 95
 1. Неокогнитрон Фукусимы (1980)
 2. LeNet (1998):
 • Идеииерархической структуры с соседних областей в сочетании с обратным
 распространением
 • Небольшое число слоев
 • Хорошо работает на MNIST
 3. AlexNet (2014):
 • Большая версия LeNet
 • Размер свертки уменьшается (с 11 ×11 до 3×3) между входом и выходом
 • Победа ImageNet2014 (по факту прорывом стала не архитектура, а то, что
 обучение сети впервые выполнялось на видеокарте)
 4. VGG-16 и VGG-19 (2014):
 • Большая версия AlexNet
 • Вместо использования больших свёрток используются комбинации меньших
 сверток (вместо 5 × 5 дважды применяется 3×3)
 • 138 и 144 млн. параметров соответственно
 5. Сеть в сети (2014):
 • Использует что-то более сложное, чем просто свёрточный слой
 • Осталась теоретической
 6. 1×1свёртка (Cascaded Cross-Channel pooling, CCCP, каскадное кросс-канальное объ
единение) позволяет играть с размерностью.
 7. Inception (2014):
 • Идея: коррелированные нейроны будут сосредоточены в небольших областях.
 Чтобы определить это, мы можем использовать свёртку 1 ×1.
 • Также попробуем искать их в областях 3×3 и 5×5.
 • Вышеперечисленные фильтры можно объединять отдельным слоем.
 8. ResNet (2015)
 23 Задачикомпьютерногозрения
 Спомощьюсвёрточных сетей можно решать задачи классификации:
Лекция 4. Свёрточные сети и работа с изображениями
 стр. 37 из 95
 Рис. 4.3: Сравнение архитектур свёрточных сетей
 • MNIST– первый набор данных для классификации. Содержит 10 классов, раз
мер изображений– 28 × 28, все изображения черно-белые и на них содержатся
 рукописные цифры.
 • Fashion-MNIST– современная замена MNIST с теми же характеристиками.
 • CIFAR-10– содержит 10 классов, изображения цветные.
 • Imagenet– датасет с 1000 классов и минимум по 1000 изображений на каждый
 класс.
 Второй класс задач– это сегметнтация, детекция и так далее:
 1. Semantic segmentation
 2. Classification + Localization
 3. Object Detection
 4. Instance segmentation
 Примечание. Такой класс задач решается сетями другими сетями (архитектурами), кото
рые могут выполнять деконволюцию (увеличение изображения).
 Примеры современных решений для решения вышеперечисленных задач:
 1. Архитектура U-Net выполняет семантическую сегментацию
 2. Задачу распознавания решает R-CNN: архитектура, которая вырезает регионы
 изображения и прогоняет их через свёрточную сеть.
 3. YOLO-архитектура, похожая на обычную свёрточную архитектуру, но в результате
 получается тензор с числом классов и пятью дополнительными значениями для
Лекция 4. Свёрточные сети и работа с изображениями
 стр. 38 из 95
 Рис. 4.4: Transposed Convolution
 каждого региона: одно из них сигнализирует что что-то в регионе удалось найти, а
 остальные четыре задают координаты рамки.
Лекция 5. Фреймворки для глубокого обучения
 стр. 39 из 95
 Лекция 5
 Фреймворки для глубокого обучения
 Лекторы: Алексей Сергеевич Забашта и Илья Шамов
 24 Краткийобзорпопулярныхфреймворков
 В Python-библиотеке sklearn есть класс sklearn.neural_network.MLPClassifier, который
 предоставляет интерфейс для работы с полносвязными нейронными сетями, однако он
 достаточно примитивен, так как не позволяет достаточно гибко настраивать сеть и её
 обучение:
 1. Методы оптимизации выбираются из небольшого списка
 2. Сходу не понятно, как настроить функцию ошибки
 Если требуется что-то более серьезное, то можно использовать библиотеку PyTorch.
 Данный фреймворк поддерживает возможность более гибкой настройки и вычисления
 на видеокарте. Основная "магия"в PyTorch происходит благодаря поддержке автомати
ческого дифференцирования и возможности гибко настраивать, для каких нейронов
 необходимо рассчитывать градиент (это позволяет замораживать слои).
 Примечание. PyTorch предоставляет интерфейс для создания собственных блоков. "Что
то обучаемое"необходимо наследовать от torch.nn.Module и переопределять в нем функ
цию forward. Функция backward и список параметров модели определяются автоматиче
ски при проходе графа вычислений.
 Альтернативой библиотеке PyTorch является TensorFlow. Он поддерживает работу со
 статическим графом вычислений и с динамическим (при использовании Eager API). Во
 второй версии библиотеки также есть возможность наследовать обучаемую модель от
 tensorflow.keras.Modelиудобноработатьсдинамическимграфом(интерфейсвTensorFlow
 аналогичный, вместо метода forward есть метод call).
Лекция 5. Фреймворки для глубокого обучения
 стр. 40 из 95
 25 ПодробнееоPyTorch
 Примечание. Между PyTorch от Facebook и TensorFlow от Google шло медиасражение и
 PyTorch выиграл, так как в TensorFlow изначально продвигали статический граф, а затем
 сделали две версии библиотеки и с этим еще надо разобраться.
 Алгоритм создания и обучение модели на PyTorch:
 1. Модель необходимо наследовать от torch.nn.Module
 2. В методе __init__ необходимо перечислить и сохранить слои модели. Для созда
ния слоев предоставляется широкий набор инструментов. Некоторые из них:
 • torch.nn.Sequential, позволяющий перечислить слои, которые должны приме
няться последовательно и инкапсулирующий в себе их применение
 • torch.nn.Conv2d — свёрточный слой
 • torch.nn.Linear — полносвязный слой
 • torch.nn.ReLU — функция активации ReLU
 • torch.nn.MaxPool2d — пулинг с операцией max
 3. Далее определяется метод forward, в котором вызываются слои модели, сохранен
ные в конструкторе (благодаря вызову слоев, будет строиться динамический граф
 вычислений).
 4. Для загрузки данных можно использовать интерфейс torch.utils.data.DataLoader,
 который может инкапсулировать постепенную загрузку данных небольшими ча
стями, которыепомещаютсявоперативнуюпамятькомпьютера(можноопределять
 свои DataLoader’ы).
 5. Для проверки структуры и оценки кол-ва параметров модели используется специ
альная библиотека torchsummaryX, предоставляющая фукнцию summary.
 Примечание. Поскольку в PyTorch используется динамический граф, в функцию
 summary также требуется передавать пример входных данных. Для трехканальных
 изображений 32 на 32 пискеля его можно создать с помощью следующего метода:
 torch.ones(1, 3, 32, 32).
 6. Для обучения модели используются оптимизаторы, например, torch.optim.Adam.
 Обычно они принимают параметры модели (model.parameters()) и гиперпарамет
ры для алгоритма оптимизации.
 7. Обучение модели частично не инкапсулировано, так что необходимо самостоя
тельно запускать внешний цикл по эпохам и внутренний по DataLoader’у, обну
лять градиент у оптимизатора методом zero_grad, запускать вычисление опти
мизируемой функции и подсчет градиента, а также оценивать результат модели
 для текущего шага и переходить к следующему шагу оптимизации (метод step
Лекция 5. Фреймворки для глубокого обучения
 стр. 41 из 95
 оптимизатора). Такой подход обеспечивает очень большую гибкость процесса обу
чения, без большой нагрузки на разработчика, так как минимальный вариант кода,
 реализующего обучение записывается в 5– 10 строчек.
 Примечание. Для логирования данных об обучении модели рекомендуется использовать
 библиотеку tensorboardX, который предоставляет интерфейс SummaryWriter. В резуль
тате создается специальный файл в отдельной директории, который можно открыть
 утилитой tensorboard (%tensorboard--logdir runs). Она удобно отображает графики и
 позволяет настраивать сглаживание, цвета и так далее.
Лекция 6. Рекуррентные сети и работа с последовательностями
 стр. 42 из 95
 Лекция 6
 Рекуррентные сети и работа с
 последовательностями
 Лектор: Сергей Борисович Муравьёв
 Примечание. На данной лекции не будут рассматриваться способы получения эмбеддин
гов для текста.
 26 Рекуррентныенейронныесети
 Биологические нейронные сети рекуррентны. Для поддержания аналогии с работой моз
га требуется работать с рекурсивными функциями. Для них нельзя напрямую применять
 автоматическое дифференцирование, так как для этого требуется ациклический граф.
 Однако функцию можно вычислить итеративно, сводя задачу к обработке временного
 ряда. Временной ряд, в свою очередь, можно свести к последовательности.
 Примечание. Первой реализацией такой идеи стала сеть Хопфилда, представляющая из
 себя реальную нейронную сеть, но тогда никто не говорил, как её обучать.
 Рекуррентные нейронные сети используются там, где есть последовательности
 • Задачи обработки и анализа текста
 • Автоматический перевод
 • Обработка аудио и видео:– Прогнозирование следующего кадра– Распознавание эмоций– Удаление шума
 • Обработка изображений (например, генерация описания)
Лекция 6. Рекуррентные сети и работа с последовательностями
 стр. 43 из 95
 Рис. 6.1: Устройство юнита рекуррентной нейронной сети
 На практике обычно последовательности сводят к рекурсии, а затем обратно к последо
вательностям. Это полезно, например, в обработке текста, так как каждый токен в тексте
 связан с последовательностью предыдущих и это, конечно, необходимо учитывать.
 Примечание. Последовательность содержит "рекурсиютак как тип Seq(X) можно опре
делить рекурсивно:
 Seq(X) = ε+X ×Seq(X).
 27 Обработкапоследовательностей
 Области применения:
 • Временные ряды
 • Естественные языки
 • Речь
 • Динамические системы
 • Изображения и видео
 • ...
 • Вцелом, произвольные последовательности
 Типы числовых последовательностей:
 • Спектральные
 • Временные
 • Частотно-временные
Лекция 6. Рекуррентные сети и работа с последовательностями
 стр. 44 из 95
 Временной ряд можно рассматривать как результат стохастического процесса в предпо
ложении о некоторых условных зависимостях:
 • Скрытые марковские модели
 • Динамические байесовские модели
 Примечание. Такие модели называются вероятностными графическими.
 Преимущества RNN(рекурентных нейронных сетей):
 • Могутаппроксимировать не только отдельно взятые функции, но и целые системы
 • Являются частью "экосистемы"нейронных сетей
 Недостатки:
 • Требуют очень много времени на обучение
 • Исчезающие/взрывающиеся градиенты
 • Всегда меняют предыдущий сигнал, который им пришел в момент времени t−1.
 Зачастую это полезно, но иногда мы не хотим его менять (однако современные
 архитектуры решают этот вопрос)
 28 Продвинутыеархитектуры
 Определение. LSTM (модуль долгой краткосрочной памяти) — разновидность архитек
туры рекуррентных нейронных сетей, предложенная в 1997 году Зеппом Хохрайтером и
 Юргеном Шмидхубером. Её идея состоит в том, что бы выделять отдельный блок для па
мяти, так как скрытые состояния плохо хранят информацию. Блок памятииспользуется
 в LSTMдля хранения глобального состояния.
 Рис. 6.2: Структура LSTM-блока
Лекция 6. Рекуррентные сети и работа с последовательностями
 стр. 45 из 95
 Рис. 6.3: а) Лента, б) фильтр забывания, в) фильтр входа, г)
 обновление памяти, д) обновление скрытого состояния, е) LSTM со
 смотровыми глазками
Лекция 6. Рекуррентные сети и работа с последовательностями
 стр. 46 из 95
 LSTM-блок состоит из следующих компонентов (рис. 6.3):
 1. Лента (conveyor belt) используется для накопления информации:
 2. Фильтр забывания (forget layer) определяет, что нужно забыть:
 ft = σ(Wf ·[ht−1,xt] + bf)
 3. Фильтр входа (input layer) определяет, что нужно записать в память:
 it = σ(Wi ·[ht−1,xt] + bi)
 Ct = tanh(Wc ·[ht−1,xt] + bc)
 4. Фильтр скрытого состояния определяет, как его изменить
 ot = σ(Wo ·[ht−1,xt] + bo)
 ht = ot ×tanhCt
 5. Опционально может содержать смотровые глазки (peephole connections) память ис
пользуется для обновления себя и скрытого состояния. Тогда остальные параметры
 пересчитываются с учетом Ct:
 ft = σ(Wf ·[Ct−1,ht−1,xt] + bf)
 it = σ(Wi ·[Ct−1,ht−1,xt] + bi)
 ot = σ(Wo ·[Ct,ht−1,xt] + bo)
 Примечание. После вычисления фильтров забывания и входа обновляется хранимое в
 памяти значение:
 Ct = ft ×Ct−1 +it × Ct
 Определение. GRU (gated restricted unit) — некоторая оптимизация LSTM, в которой
 меньше параметров и отсутствует выходной фильтр. Структура блока GRU представлена
 на рисунке 6.4. Значения в блоке пересчитываются по следующим правилам:
 zt = σ(Wz ·[ht−1,xt])
 rt = σ(Wr ·[ht−1,xt])
 ht = tanh(W ·[rt ×ht−1,xt])
 ht = (1−zt)×ht−1 +zt ×ht
 Преимущества LSTM:
Лекция 6. Рекуррентные сети и работа с последовательностями
 стр. 47 из 95
 Рис. 6.4: GRU
 • Могут работать с удаленными во времени зависимости
 • Нет затухания и взрыва градиент
 Недостатки:
 1. Требуют много времени для обучения
 2. Всё еще забывают длинные зависимости
 29 Большесвязей
 Примечание. Иногда предыдущей информации может быть не достаточно для правиль
ной обработки текущего сигнала. Например,
 He said "Teddy bears are on sail!"
 He said "Teddy Roosevelt was a great President!"
 Рис. 6.5: biRNN
 Определение. Двунаправленная RNN (Bidirectional RNN, biRNN) представляет собой
 две однонаправленные рекуррентные сети, одна из которых обрабатывает входную по
следовательность в прямом порядке, а другая — в обратном. Таким образом, для каждого
Лекция 6. Рекуррентные сети и работа с последовательностями
 стр. 48 из 95
 Рис. 6.6: Seq2Seq архитектура
 элемента входной последовательности считается два вектора скрытых состояний, на
 основе которых вычисляется выход сети. Благодаря данной архитектуре сети доступна
 информацияоконтексте как из прошлого, так и избудущего, что решает проблему одно
направленных рекуррентных сетей. Для обучения biRNN используются те же алгоритмы,
 что и для RNN.
 Примечание. На практике у biRNN всё еще есть проблемы с определением контекста.
 Определение. Многослойная рекуррентная нейронная сеть — архитектура, состоящая
 из нескольких слоев, каждый из которых представляет собой RNN.
 Классификация рекурентных нейронных сетей:
 1. one to one — один вход, один выход. Примерами являются сети, которые мы обсуж
дали до RNN.
 2. one to many — один вход, несколько выходов. Например, задача генерации аудио
 по жанру (один токен на входе)
 3. many to one — много входов, один выход. Например, классификация тональности
 текста.
 4. many to many:
 (a) Схема перевода, выходы в которой не соотнесены со входами,
 (b) Архитектура "многие ко многимно выходы соотнесены со входами. Пример
классификация каждого слова.
 Определение. Seq2Seqархитектура—архитектура,состоящаяизкодировщика(encoder)
 и декодировщика (decoder). Получает на вход одну последовательность, возвращает
 другую. Ячейки в ней могут быть любой сложности. Между кодировщиком и декодиров
щиком передается вектор, кодирующий последовательность.
Лекция 6. Рекуррентные сети и работа с последовательностями
 стр. 49 из 95
 Рис. 6.7: Механизм внимания
 Примечание. В качестве ячеек в Seq2Seq архитектуре можно использовать LSTM ячейки,
 а в декодировшике использовать softmax по выходу, но работает это на практике очень
 плохо (справедливо и для остальных рекурентных блоков):
 • Кодировщик может напрочь забыть начало длинных предложений.
 • Жадное декодирование, когда модель пытается перевести каждое слово так, как оно
 есть (может решаться методом BeamSearch, когда поддерживается набор предпо
ложений- например, три наиболее вероятные гипотезы, с ними далее строится
 дерево, например, на пять токенов вперед и уже только потом выбирается наиболее
 вероятная и корректная версия).
 30 Механизмвнимания
 Предложение очень сложно (невозможно) закодировать одним вектором. Поэтому для
 того чтобы его обработать, можно смотреть не на одно скрытое состояние, а на все скры
тые состояния. Во-первых, это может частично решить проблему забывания длинных
 последовательностей. Во-вторых, в контексте языковых моделей это может улучшить
 работу с контекстом.
 Определение. Механизм внимания (attention mechanism, attention model) — техника
 используемая в рекуррентных нейронных сетях и свёрточных нейронных сетях для
 поиска взаимосвязей между различными частями входных и выходных данных.
 Анализ механизма внимания:
 • Обучается так же, как и другие блоки
Лекция 6. Рекуррентные сети и работа с последовательностями
 стр. 50 из 95
 Рис. 6.8: Обобщенный механизм внимания в RNN
 Рис. 6.9: Пример работы Seq2Seq сети с механизмом внимания
Лекция 6. Рекуррентные сети и работа с последовательностями
 стр. 51 из 95
 • Позволяет обрабатывать более длинные последовательности
 • Вцелом, улучшает производительность
 • Можетприменяться в произвольных сетях
 • Добавляет больше параметров
 31 Тансформеры
 Основная идея:
 • Механизм внимания находит похожести между векторами.
 • Попробуем отказаться от скрытых состояний и просто будем искать похожесть
 внутри входящей последовательности.
 Такой подход называется само-внимание (self-attention).
 Определение. Трансформер (transformer) — архитектура глубоких нейронных сетей,
 основанная на механизме внимания без использования рекуррентных нейронных сетей.
 Самое большое преимущество трансформеров по сравнению с RNN заключается в их
 высокой эффективности в условиях параллелизации. Впервые модель трансформера
 была предложена в статье "Attention is All You Need"от разработчиков Google в 2017 году.
 Устройство трансформера состоит из кодирующего и декодирующего компонентов. На
 вход принимается некая последовательность, создается ее векторное представление,
 прибавляется вектор позиционного кодирования, после чего набор элементов без учета
 порядка в последовательности поступает в кодирующий компонент (параллельная обра
ботка), а затем декодирующий компонент получает на вход часть этой последовательно
сти и выход кодирующего. В результате получается новая выходная последовательность.
 Примечание. Кодирующий компонент выглядит как стек энкодеров, а декодирующий,
 как стек декодеров.
 Строение кодировщика:
 1. Self-Attention
 2. Feed Forward
 Строение декодировщика:
 • Self-Attention
 • Encoder-Decoder Attention — слой, помогающий декодировщику фокусироваться на
 релевантных частях последовательности
 • Feed Forward
Лекция 6. Рекуррентные сети и работа с последовательностями
 стр. 52 из 95
 Рис. 6.10: Упрощенная схема трансформера
 Примечание. Все блоки в трансформерах одинакового размера, но имеют разные пара
метры.
 Слова в трансформерах обрабатываются не последовательно, но не независимо:
 1. Составляется векторное представление слов (эмбеддинги)
 2. Каждое слово по отдельности проходит через два подслоя кодировщика:
 3. Они проходят через общий слой Self-Attention
 4. Каждое из них проходит через свой Feed Forward подслой.
 Определение. Self-Attention — разновидность механизма внимания, задачей которой
 является выявление закономерности между входными данными. Для каждого вектора
 xi выполняются следующие действия:
 1. Рассчитывается запрос (query) qi = Qxi
 2. Рассчитывается ключ (key) ki = Kxi
 3. Рассчитывается значение (value) vi = V xi
 4. Рассчитывается score, равный qi · ki
 5. Score делится на 8 (корень из длины векторов запроса, ключа и значения)
 6. Для всех слов высчитывается Softmax от значений с предыдущего шага
 7. Результат умножается на вектор vi
Лекция 6. Рекуррентные сети и работа с последовательностями
 стр. 53 из 95
 Рис. 6.11: Механизм Self-Attention
 Определение. Multi-headed self-attention — улучшенная модификация self-attention.
 Слойвниманияснабжаетсямножеством«подпространствпредставлений»(representation
 subspaces). Теперь у нас есть не один, а множество наборов матриц запроса/ключа/зна
чения. Каждый из этих наборов создается случайным образом. Далее после обучения
 каждый набор используется для отображения входящих векторов в разных подпростран
ствах представлений. Также появляется способность модели фокусироваться на разных
 аспектах входной информации.
 То есть параллельно независимо несколько раз делаем attention. Потом результат каж
дого attention по элементам конкатенируем, затем сжимаем получившуюся матрицу и
 получаем для каждого элемента свой вектор той же размерности.
 Дополнительные трюки, которые позволяют улучшить работу трансформеров:
 • Positionsl encoding — способ передать модели информацию о порядке элементов
 последовательности путемприбавленияспециальныхметокквекторувходныхэле
ментов. Позиции кодируются векторами pi таким образом, чтобы при увеличении
Лекция 6. Рекуррентные сети и работа с последовательностями
 стр. 54 из 95
 |i − j| увеличивался и ||pi − pj||. Пример такого кодирования:
 
 
 
 
 
 
 
 
 
 p(i,s) =
 • Layer normalization
 • Skip-connections
 
 
 
 
 
 
 
 sin
 cos
 i·10000
 
 i·10000
 • Masked multi-head attention
 Достоинства трансформеров:
 
 −2k
 d
 −2k
 d
  еслиs=2k
 
  еслиs=2k+1
 • Есть возможность распараллеливания обучение модели
 • State-of-the-art качество работы
 • Потенциально интерпретируемые результаты
 Недостатки:
 • Очень много параметров
 • Нестабильно обучается
 • Требуется фиксированная длина предложений (на текущий момент уже не так
 актуально)
Лекция 7. Анализ текста
 стр. 55 из 95
 Лекция 7
 Анализ текста
 Лектор: Илья Шамов
 32 Определениетекста ипостроение эмбеддингов
 Определение. Текст — это набор слов. Если можем представить слово, то сумма пред
ставлений слов будет представлением текста.
 Основные тезисы об эмбеддингах текста:
 • One-hot encoding слов слишком ресурсоёмкий
 • Ручная разметка слов невозможна
 • Слово определяется контекстом
 • Дляполучения эмбеддингов можно использовать следующие модели: skip-gram,
 CBOW
 Задачи, связанные с анализом текстов:
 • Классификация
 • Моделированиеязыка—построениематематической моделиязыка, эмулирующей
 естественный язык
 • Генерация
 • Перевод
 • Выделение эмоций
 • Ответы на вопросы, ведение диалога
 • Суммаризация
Лекция 7. Анализ текста
 стр. 56 из 95
 • Поиск
 • Анализ
 33 Предобработка текста средствами Python
 На этапе предварительной обработки текста можно выполнить следующие шаги (не все
 они являются оптимальными):
 • Удалить лишние символы, такие как знаки пунктуации.
 • Посмотреть на статистику наиболее популярных слов. Скорее всего, в топе будут
 предлоги, артикли и т.д. Такие слова называются стоп-словами. Часть из них
 могут специфичными для набора данных.
 • Далееможноочиститьтекст от общихдляязыкастоп-слов, выполнить это можно с
 помощьюпроверкивсехсловнаналичиевnltk.corpus.stopwords для необходимого
 языка (например, в stopwords.words(’english’)).
 • После предыдущего шага в топе останутся специфичные для набора данных стоп
слова, может быть полезно избавиться и от них.
 • Очень часто текст содержит разные формы одного и того же слова, полезно приме
нять один из следующих подходов.
 Определение. Стемминг (nltk.stem.PorterStemmer) — замена слов на его основу.
 Определение. Лемматизация (nltk.stem.WordNetLemmatizer) — процесс приве
дения словоформы к лемме, то есть её нормальной (словарной) форме.
 Примечание. На практике для глубокого обучения чаще используют стемминг, так
 как он сильнее уменьшает размер словаря.
 34 Алгоритмыклассического машинного обучения
 Определение. TF-IDF (TF — term frequency, IDF — inverse document frequency) — ста
тистическая мера, используемая для оценки важности слова в контексте документа,
 являющегося частью коллекции документов или корпуса. Вес некоторого слова пропор
ционален частоте употребления этого слова в документе и обратно пропорционален
 частоте употребления слова во всех документах коллекции.
 TF-IDF строит эмбеддинги для текстов. Их размер равен размеру словаря и может быть
 порядка десятков тысяч элементов. По факту каждый тексты редко содержит все слова
 из словаря, поэтому если хранить такие эмбеддинги в разреженной матрице, то проблем
 не будет.
Лекция 7. Анализ текста
 стр. 57 из 95
 Определение. Latent Dirichlet allocation (LDA, Латентное размещение Дирихле) — при
меняемая в машинном обучении и порождающая unsupervised модель, позволяющая
 объяснять результаты наблюденийспомощьюнеявныхгрупп,благодарячемувозможно
 выявление причин сходства некоторых частей данных. Например, если наблюдениями
 являются слова, собранные в документы, утверждается, что каждый документ представ
ляет собой смесь небольшого количества тем и что появление каждого слова связано с
 одной из тем документа.
 Верхнеуровнево LDA описывается с помощью 2-х наборов параметров:
 • α—вероятность каждого документа принадлежать каждому из топиков
 • β —вероятность каждого слова принадлежать каждому из топиков
 Модель случайно делит тексты на топики и далее с помощью градиентного спуска
 настраивает α и β. Таким образом она увеличивает правдоподобие своей оценки. Данная
 модель уже реализована: gensim.models.LdaMulticore.
 35 Обучениеэмбеддингов слов
 Самыйпростойспособпостроения эмбеддингов для текстов — это one-hot encoding, одна
ко он несет слишком мало информации и на практике мало где применим. Рассмотрим
 более умные способы построения векторного представления текстов.
 Определение. CBOW(Continuous Bag of Words) — архитектура сети, решающей задачу
 определения наиболее вероятного слова по контексту.
 Определение. Word2Vec — способ построения сжатого пространства векторов слов,
 использующий нейронные сети. Принимает на вход большой текстовый корпус и сопо
ставляет каждому слову вектор. Сначала он создает словарь, а затем вычисляет векторное
 представление слов. Векторное представление основывается на контекстной близости:
 слова, встречающиесявтекстерядомсодинаковымисловами(аследовательно,имеющие
 схожий смысл).
 Еще один способ построения эмбеддингов — использование глубоких сетей с архитек
турой, разделенной на кодировщик и декодировшик. Если оставить только encoder —
 то с помощьюнего можно строить эмбеддинги текстов. Примером такой архитектуры
 являются трансформеры, рассмотренные на предыдущей лекции.
 Определение. BERT—этомногослойныйдвунаправленныйкодировщиктрансформер.
 В данной архитектуре используется двунаправленный механизм self-attention. Модель
 используется в совокупности с некоторым классификатором, на вход которого подается
 результат работы BERT — векторное представление входных данных. В основе обучения
 модели лежат две идеи:
 1. 15% слов заменяются масками и BERT учится их предсказывать.
Лекция 7. Анализ текста
 стр. 58 из 95
 2. BERT учится определять, может ли одно предложение идти после другого.
 Примечание. Вбиблиотеке transformers есть такие классы, как BertTokenizer и BertModel.
Лекция 8. Анализ звука
 стр. 59 из 95
 Лекция 8
 Анализ звука
 Лектор: Илья Шамов
 36 Короткоозвуке
 Применение машинного обучения в анализе звука:
 • Классификация аудио
 • Распознавание речи
 • Удаление шума и улучшение качества аудио
 • Получение информации об аудио (классификация музыкальных инструментов
 или настроения)
 • Синтез речи, музыки
 Звук происходит из-за вибраций различных объектов. Эти вибрации заставляют моле
кулы воздуха осциллировать и собираться в группы. Именно этот процесс изменяет
 плотность молекул воздухе (изменение воздушного давления), порождая звуковые вол
ны.
 Звуковой файл на любом вычислительном устройстве хранится как список или списки
 чисел. У каждого аудио-файла есть следующие мета-параметры:
 • Количество каналов (например, моно или стерео)
 • Частота дискретизации (sampling rate) — число измерений звука в секунду
 Примечание. Чем выше частота звука (не путать с частотой дискретизации) — тем выше
 звук мы воспринимаем.
 Примечание. Две различные частоты воспринимаются человеком одинаково, если они
 различаются менее чем на наименьшую ближайшую степень двойки.
Лекция 8. Анализ звука
 стр. 60 из 95
 Дляработысозвукомполезнопомнитьотом,чторазныеживыесуществавоспринимают
 звук в разном диапазоне:
 • Человек: 20– 20’000 Гц
 • Слон: 14– 12’000 Гц
 • Кот: 48– 75’000 Гц
 • Собака: 64– 45’000 Гц
 • Мышь:1’000– 70’000 Гц
 • Летучая мышь: 7’000– 200’000 Гц
 Примечание. В этом списке кроется ответ, почему все современные аудио-носители
 используют частоту дискретизации равную 44100 или 48000.
 Определение. Частота Найквиста — верхняя граница на частоту, которая может быть
 закодирована с использованием какой либо частоты дискретизации (SR) без серьезных
 артефактов:
 fk = SR
 2 .
 Для 44’100 получаем 22’050, что чуть больше, чем 20’000
 Примечание. На практике, очень не рекомендуется использовать частоты, которые пре
вышают частоту Найквиста, так как преобразования, направленные на работу с аудио
 будут показывать некорректное поведение.
 37 Признаковоеописание аудио-сэмплов
 Если работать с сырым аудио файлом как с признаковым описанием, то мы можем в
 какой-то момент столкнуться с проблемой, когда у аудио-файлов разной длины разная
 размерность признакового описания. В связи с этим нам необходимо разработать более
 робастные (выбросоустойчивые) инструменты признакового представления для аудио.
 Основные пространства для формирования признаков для аудио:
 1. Time domain: Признаки этой группы извлекают данные напрямую из аудио
сигнала. Работа идет в пространстве "Время-Амплитуда".
 2. Frequencydomain:Дляэтихпризнаковхарактеренпереходизпространства"Время
Амплитуда"в пространство "Частота- Сила (Magnitude)". Самый очевидный пример
 такого признака это преобразование Фурье, которое переводит аудио-файл в в
 частотное пространство, с которым уже можно работать как с признаковым описа
нием. Однако, самый важный недостаток такого подхода заключается в том, что
 мыполностью избавляемся от привязки ко времени, что делает недоступным для
 нас анализ определённой части аудио-сигнала.
Лекция 8. Анализ звука
 стр. 61 из 95
 3. Time-frequency domain:
 • Спектрограммы
 • Спектрограммы Мела
 • MFCC(Mel Frequency Cepstral Coefficients)
 • Constant Q-transform
 Определение. Преобразование Фурье — преобразование, которое позволяет разбить
 аудио-сигнал на компоненты разных частот и посчитать их амплитуду. В Python реали
зовано в scipy.fft.fft и scipy.fft.fftfreq.
 Определение. Спектрограмма — изображение (тепловая карта), показывающее за
висимость спектральной плотности мощности сигнала от времени. Спектрограммы
 применяются для идентификации речи, анализа звуков животных и так далее. Оси x, y,
 z спектрограммы описывают время, частоту и магнитуду соответственно.
 Параметры, требуемые для построения спектрограммы:
 • Величину окна
 • Величину сдвига
 • Частоту дискретизации (берется из мета-информации к аудио-файлу).
 Примечание. Для отображения спектрограммы можно использовать функцию
 librosa.display.specshow.
 Определение. Спектрограммы Мела– модификация спектрограммы, которая учиты
вает от факт, что человеческое ухо не может отличать некоторые частоты друг от друга.
 Примерная формула восприятия частот (Mel scale):
 M =1125·ln 1+ f
 700
 Примечание. Для построения спектрограммы Мела можно использовать функцию
 librosa.feature.melspectrogram.
 Определение. MFCC (Mel Frequency Cepstral Coefficients) — мел-кепстральные коэффици
енты. Определяются следующим алгоритмом:
 1. Вычисляется спектрограмма Мела,
 2. Спектрограмма Мела поэлементно логарифмируется,
 3. Вычисляется дискретное косинусное преобразование (Discrete Cosine Transform,
 DCT),
 4. Полученные значения амплитуд от полученного спектра являются MFCC.
Лекция 8. Анализ звука
 стр. 62 из 95
 Примечание. Ось y на MFCC — это коэффициенты (безразмерная величина).
 Примечание. Размерность MFCC значительно меньше, чем у спектрограмм.
 Примечание. Для построения MFCC можно использовать функцию librosa.feature.mfcc.
Лекция 9. Кластеризация
 стр. 63 из 95
 Лекция 9
 Кластеризация
 Лектор: Сергей Борисович Муравьёв
 38 Базоваяпостановка задачи кластеризации
 Задача классификации состоит в том, чтобы разделить набор объектов на группы так,
 чтобыобъектывэтихгруппахимелипохожиесвойства.Похожестьформализуетсямерой.
 Определение. Пусть задан набор данных D, состоящий из объектов на X, ρ : X ×X →
 [0; +∞) — метрика на X. Тогда задача кластеризации состоит в поиске алгоритма
 α : X →Y,гдеY–множествокластеров. Выделяют два вида кластеризации:
 • Жесткая кластеризация — объект принадлежит ровно одному кластерам.
 • Мягкаякластеризация — объект принадлежит одному или многим кластерам.
 Заметим, что данная постановка задачи математически некорректна по следующим
 причинам:
 1. Нет правильной постановки задачи
 2. Нет универсального критерия качества
 3. Нет универсальной меры расстояния между объектами (следствие теоремы Клейн
берга)
 4. Число кластеров обычно неизвестно
 Примеры кластеров:
 • Явно разделимые
 • Полосы
 • С"мостами"
Лекция 9. Кластеризация
 стр. 64 из 95
 • Сшумами
 • Смесь распределений
 • Нет "кластеров"
 Практическое применение кластеризации:
 • Биология и медицина:– Анализ последовательностей– Медицинская визуализация (КТ и МРТ)
 • Социальные науки– Анализ криминала
 • Информационные технологии:– Сегментация изображения
 • Маркетинг:– Целевые группа
 • Анализ текста
 • Социальные сети:– Выделение сообществ
 39 Оценкакачествакластеризации
 Определение. Внутренние меры — меры, которые не используют внешней информа
ции оструктуре разбиения (только X и построенное разбиение Y. Примеры:
 • Компактность кластеров (Cluster Cohesion)
 • Отделимость кластеров (Cluster separation)
 • Индекс Дана (Dunn Index) и его вариации
 • Индекс Sdbw
 • Силуэт (Silhouette)
 • Индекс Calinski-Harabasz
 • Индекс C
 • Индекс Дэвида-Болдуина (Davies-Bouldin Index)
 • Score function
Лекция 9. Кластеризация
 стр. 65 из 95
 • Индекс γ
 • Индекс COP
 • Индекс CS
 • Индекс Sym иего вариации
 • Индекс SV
 • Индекс OS
 Определение. Внешние меры —меры, которые используют для оценки Y и Y. Приме
ры:
 • Модифицированные TP, FP, FN, TN
 • Индекс Rand
 • Индекс Adjusted Rand
 • Индекс Жаккара (Jaccard Index)
 • Индекс Фоулкса-Мэллова (Fowlkes-Mallows Index)
 • Hubert Г statistic
 • Индекс ϕ
 • Minkowski Score
 • Индекс Гудмэна-Крускала (Goodman-Kruscal Index)
 • Purity
 • Variation of Information
 Примечание. Не рекомендуется использовать внешние меры для оценки алгоритмов.
 40 Эвристические алгоритмы кластеризации
 40.1 k-Means
 Определение. Метод K-средних (Алгоритм Ллойда) — метод решения задачи кластери
зации. Работает по следующему алгоритму:
 1. Выбираются K центров случайным образом.
 2. Точка относится к тому кластеру, к центру которого она ближе всего.
 3. Рассчитываются средние по координатам точек для каждого кластера.
 4. Центры кластеров перемещаются к средним координатам.
Лекция 9. Кластеризация
 стр. 66 из 95
 5. Повторяются шаги 2- 4, пока центры кластеров не перестанут перемещаться.
 Подводные камни:
 • Неправильный выбор числа кластеров.
 • Неправильная инициализация (выбор случайных центров).
 Определение. Методлоктя(elbowmethod)—алгоритмвыборачислакластеров:строится
 зависимость суммы квадратов расстояний от каждой точки до ближайшего центра и
 зависимости от k– числа кластеров, а когда график выходит на плато, можно считать,
 что найдено неплохое значение k.
 Примечание. На практике иногда на графике не видно локтя.
 Определение. SilhoutteAnalysis(анализсилуэтов)—алгоритмприкоторомдлякаждой
 точки из набора данных рассчитывается среднее расстояние до точек своего кластера ai,
 среднее расстояние до точек ближайшего другого кластера bi и коэффициент:
 bi −ai
 max(ai,bi)
 Значение коэффициента в районе 0 означает, что точка на границе кластеров, в районе 1– точка глубоко в своем кластере, меньше 0– точка неправильно кластеризована.
 Недостатки k-Means:
 1. Кластеры стремятся быть в форме шара (многоугольники Вороного)
 2. Кластеры стремятся быть одинакового размера.
 Таким образом, k-Means– это быстрый и простой алгоритм всего лишь с одним ги
перпараметром (хотя наличие гиперпараметра само по себе уже минус), однако он не
 умеет обрабатывать кластеры сложной формы и содержит случайность, а значит, мо
жет выдавать разные результаты при нескольких запусках. Де-факто является самым
 часто применяемым алгоритмом для анализа данных, смело может быть использован в
 качестве бейз-лайна.
 Примечание. Можно сдвигать центры не к среднему, а к медиане. Такой метод называ
ется k-Medians. Он более устойчив к выбросам, однако скорость его работы падает при
 большом количестве данных.
 40.2 MeanShift
 Определение. Mean-Shift Clustering– метод решения задачи кластеризации. Работает
 по следующему алгоритму:
 1. Выбирается окно радиусом R.
 2. Окно сдвигается в сторону центра точек, которые в него попали.
Лекция 9. Кластеризация
 стр. 67 из 95
 3. Окно двигается в сторону уплотнения точек, пока не достигает вершины.
 4. Таких оконоченьмного, ихинициализируютсеткой, чтобыкаждаяточка попадала
 хотя бы в окно.
 5. Пересекающиеся окна удаляются.
 6. Точки относятся к тому кластеру, центр которого находится ближе к ним.
 Преимущества Mean-Shift:
 • Число кластеров определяется автоматически
 • Размеры кластеров могут быть сильно разными
 • Центр кластера всегда в точке с локальным максимумом плотность, что обеспечи
вает устойчивость к выбросам.
 Минусы:
 • Кластеры по-прежнему шарообразные (в контексте метрических пространств)
 • Выбор размера окна может быть нетривиальной задачей и фактически этот пара
метр неявно задает число кластеров.
 40.3 DBSCAN
 Определение. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) —
 наиболее яркий представитель алгоритмов плотностной кластеризации.
 1. Алгоритм начинает работу с произвольной еще не посещенной точки.
 2. Если в ε-окрестности находится хотя бы minPoints других точек, то отмечаем дан
ную точку как часть кластера и всю её ε-окрестность считаем этим кластером.
 3. Отмечаем точку как посещенную.
 4. Повторяем шаги 1- 3 для всех точек.
 5. Точки, которые не попали ни в какой кластер, считаются шумом.
 Плюсы DBSCAN:
 • Число кластеров определяется автоматически
 • Формыкластеров могут быть любыми
 Минусы:
 • Требует настройку двух гиперпараметров, а не одного
 • Дляданныхсразной плотностью в кластерах подбор гиперпараметров может быть
 сложным
Лекция 9. Кластеризация
 стр. 68 из 95
 40.4 EMGMM
 Определение. EMGMM(Expectation-Maximization Clustering using Gaussian Mixture Models
 —алгоритм кластеризации, который пытается гауссовскими распределениями покрыть
 точки плоскости, в остальном работает как k-Means.
 По сравнению с k-Means EM GMM имеет следующие плюсы:
 • Большая гибкость модели, так как кластеры могут быть эллиптической формы,
 • Используются вероятности, то есть точка может принадлежать нескольким класте
рам, а значит, можно стоить мягкую кластеризацию.
 40.5 Кластеризация на основе графов
 Довольно старая и тривиальная идея заключается в том, что бы представить объекты как
 вершины полносвязного графа G = (v,e), в котором длины ребер равны расстояниям
 между объектами ρ(v,u). Алгоритм кластеризации в данном случае заключается в удале
нии ребер, длина которых больше некоторого фиксированного радиуса R. Компоненты
 связности после удаления ребер и являются кластерами.
 40.6 Иерархическая кластеризация
 При иерархической кластеризации строятся дендрограммы — деревья иерархии класте
ров. Число кластеров определяется высотой дерева. Существуют два подхода:
 1. Агломеративный, когда маленькие кластеры постепенно объединяются в большие,
 2. Разделяющий, когда изначально один большой кластер постепенно дробится.
 Примечание. Агломеративный подход является значительно более эффективным, так
 как при разделении классов у нас есть слишком много вариантов это делать (порядка
 чисел Стирлинга и нет хорошего критерия выбора оптимального способа).
 Определение. HAC (Hierarchical Agglomerative Clustering — алгоритм кластеризации,
 который изначально считает каждую точку отдельным кластером и на каждом шаге
 сливает два ближайших кластера в один и повторяет этот процесс, пока не останется
 всего один кластер. Для подсчета расстояния между кластерами можно использовать
 довольно много различных подходов, вот некоторые из них:
 • Average linkage — среднее расстояние между точками обоих классов
 • Расстояние между двумя ближайшими точками
 • Расстояния между центроидами кластеров
 Основной минус данного алгоритма заключается в том, что время его работы O(n3), зато
 он не требует задавать число кластеров и не чувствителен к выбору метрики.
Лекция 9. Кластеризация
 стр. 69 из 95
 Примечание. Данныйалгоритмпозволяетсначалавыполнитьпостроениедендрограммы
 единожды, а затем выбирать число кластеров без повторного обучения.
 40.7 Спектральная кластеризация
 Определение. Спектральнойкластеризация—этонабортехник,которыеиспользуют
 спектр (собственные значения) матрицы сходства данных для осуществления снижения
 размерности перед кластеризацией в пространствах меньших размерностей. Матрица
 сходства подаётся в качестве входа и состоит из количественных оценок относительной
 схожести каждой пары точек в данных.
 41 Нейросетевыеметодыкластеризации
 41.1 Сквозная идвухэтапная кластеризация
 Определение. Двухэтапная кластеризация (two-stage), как следует из названия, про
изводится в два этапа. Сначала необходимо получить эмбеддинги из замороженной
 глубокой нейросети или Word2Vec, а затем обработать их классическим (эвристическим)
 методом кластеризации.
 Преимущества такого подхода состоят в том, что все необходимые компоненты уже
 реализованы во многих библиотеках, а кластеризация интерпретируема. При этом двух
этапная кластеризация страдает от "проклятия размерности"и не учитывает априорную
 информацию.
 Определение. Сквозные методы (end-to-end), в частности глубокая кластеризация
 (deep clustering) — это подход к решению задач кластеризации с помощью двухголовых
 нейронныхсетей.Внихестькодировщик,перваяголова–декодировщикивтораяголова
 —модуль, отвечающий за кластеризацию.
 Такие модели хорошо работают с высокой размерностью, учатся представлять данные в
 пространстве малой размерности, хорошо масштабируются, имеют довольно высокую
 скорость и точность. Однако гиперпараметры в них настраиваются нетривиально, к
 каждой задаче требуется строить сложную в реализации модель, а результаты тяжело
 интерпретировать.
 Примечание. Функция ошибки в сквозных методах также усложняется, так как является
 линейной комбинацией ошибки сети и ошибки кластеризации.
 Примечание. Строить архитектуру глубокой нейронной сети для глубокой кластериза
ции можно на основе автокодировщика, на основе генеративной модели и с прямой
 оптимизацией кластеров.
Лекция 10. Выбор признаков
 стр. 70 из 95
 Лекция 10
 Выборпризнаков
 Лектор: Алексей Сергеевич Забашта
 42 Уменьшениеразмерности
 Определение. Задача уменьшения размерности формулируется следующим образом:
 F = (f1,...,fn). Необходимо построить множество признаков G = (g1,...,gk) : k < n
 (часто k << n), переход к которым сопровождается наименьшей потерей информации.
 Цели уменьшения размерности:
 1. Ускорение обучения и обработки
 2. Борьба с шумом и мультиколлинеарностью
 3. Интерпретация и визуализация данных
 Определение. Проклятие размерности (curse of dimensionality) — это набор проблем,
 возникающих с ростом размерности:
 1. Увеличиваются требования к памяти и вычислительной мощности
 2. Данные становятся более разреженными
 3. Проще найти гипотезы, не имеющие отношения к реальности
 Пример. Когда-то ВВС некоторой страны решили, что удобнее шить форму не под каж
дого пилота, а под усредненного и брать только тех, кто подходит под эти параметры.
 Однако на практике оказалось, что поскольку параметров у такой формы много– веро
ятность того, что хоть кто-то подойдет под все из них крайне мала.
 Уменьшение размерности — это один из шагов в предобработке данных:
 • Меньшепамяти для хранения
Лекция 10. Выбор признаков
 стр. 71 из 95
 • Уменьшение времени обработки
 • Увеличение качества обработки
 • Понимание природы признаков
 Рис. 10.1: Методы уменьшения размерности
 Определение. Выбор признаков (feature selection) — методы, для которых G ⊂ F. Они:
 • быстро работают;
 • не могут "выдумывать"сложных признаков.
 Их цель:
 1. Уменьшение числа ресурсов, требуемых для обработки больших наборов данных;
 2. Поиск новых признаков.
 Определение. Извлечение признаков (feature extraction) — все остальные методы, в
 том числе даже те, у которых k > n:
 • вцелом, дольше работают;
 • могут извлекать сложные признаки.
 Их цель:
 1. Уменьшение переобучения и улучшение качества предсказания;
 2. Улучшение понимания моделей.
 Зачем вообще избавляться от признаков? Потому что они бывают ненужными:
 Определение. Избыточные (redudant) признаки не привносят дополнительной ин
формации относительно остальных признаков.
 Определение. Нерелевантные (irrelevant) признаки — это неинформативные при
знаки.
Лекция 10. Выбор признаков
 стр. 72 из 95
 Методы выбора признаков подразделяются на:
 1. Встроенные методы (embedded)
 2. Фильтрующие методы (filter)
 • Одномерные (univariate)
 • Многомерные (multivatiate)
 3. Методы-обертки (wrapper)
 • Детерминированные (deterministic)
 • Стохастические (stochastic)
 4. Гибридные и ансамблирующие методы
 43 Встроенныеметоды
 Рис. 10.2: Схема встроенного метода
 Определение. Встроенные методы (embeeded methods) — это методы выбора призна
ков, при которых этот выбор осуществляется в процессе работы других алгоритмов
 (классификаторов и регрессоров):
 • Опираются на конкретный алгоритм и его реализацию
 • Специфичны для каждого алгоритма
 Пример. SVM-RFE (Support Vector Machine Recursive Feature Elimination) — типичный встро
енный метод:
 • Обучается SVM на тренировочном подмножестве набора данных.
Лекция 10. Выбор признаков
 стр. 73 из 95
 • Веса признаков устанавливаются равными модулям соответствующих коэффици
ентов.
 • Признаки ранжируются согласно их весам.
 • Некоторое число признаков с наименьшими весами выбрасываются.
 • Шаги1-4повторяют, пока не останется необходимое число признаков.
 Данный метод считается встроенным именно потому что он завязан на внутреннем
 устройстве SVM. А именно — на существовании некоторых коэффициентов по абсолют
ной величине которых можно оценивать важность признаков.
 Пример. Выбор признаков на основе случайного леса — еще один встроенный метод. Он
 учитывает число и глубину вхождений признака в деревья.
 44 Методы-обертки
 Рис. 10.3: Схема метода-обертки
 Определение. Методы-обертки ( wrapper methods) — методы выбора признаков, ко
торые используют алгоритм (классификатор или регрессор) для оценки качества полу
чаемого подмножества признаков и алгоритмы дискретной оптимизации для поиска
 оптимального подмножества признаков.
 Классификация методов оберток
 1. Детерминированные:
 • SFS (sequential forward selection)– итеративный алгоритм, который посте
пенно выбирает наиболее полезные признаки. Сначала рассматриваются все
 признаки по отдельности: f1, ..., fn. Среди них выбирается тот, на котором
 классификатор или регрессор показывает наилучший результат: fi1
 . Затем
 рассматриваются пары признаков fi1
 ,fi2=i1
 . Среди всех таких пар выбирается
 лучшая и так далее.
 • SBE (sequential backward elimination) — алгоритм аналогичный SFS, но начина
ющийсполного набора признаков и выкидывающий на каждой итерации
 наименее полезный признак– то есть тот, без которого классификатор или
 регрессор показывает наилучший результат.
Лекция 10. Выбор признаков
 стр. 74 из 95
 • Перестановочная полезность (permutation importance) — метод, который обу
чает какой-то алгоритм (классификатор или регрессор) на некоторой части D
 набора данныхcиспользованиемвсехпризнаков, далее для каждого признака
 рассчитывает меру его полезности µ(fi) согласно следующим правилам:
 (a) Строится новый набор данных Di, который получается из D перестанов
кой значений fi между объектами.
 (b) µ(fi) = L(D)−L(Di), где L–функция,которая принимает набор данных
 и возвращает величину ошибки предсказания обученной ранее модели
 на этом наборе данных. Таким образом, если ошибка увеличивается, то
 значение µ(fi) будет падать.
 Признаки, мера µ перестановочной полезности которых оказалась наимень
шей, выкидываются.
 Примечание. Признаки нужно "портить"именно перестановкой, чтобы сохра
нить статистические показатели, такие как E и D.
 2. Стохастические — сводят задачу выбора признаков к задаче оптимизации в про
странстве бинарных векторов:
 • Поиск восхождением на холм (почти аналогичен SFS)
 • Генетические алгоритмы
 Преимуществами методов-оберток являются:
 • Более высокая точность, чем у фильтров;
 • Использование отношений между признаками;
 • Оптимизация качества предсказательной модели в явном виде.
 Однако, у них есть ряд недостатков:
 • Очень большое время работы;
 • Склонность к переобучению при неправильной работе с разбиением набора дан
ных.
 45 Фильтры
 Рис. 10.4: Схема фильтрующих методов
Лекция 10. Выбор признаков
 стр. 75 из 95
 Определение. Фильтры (filter methods) — методы, которые оценивают качество от
дельных признаков или подмножеств признаков и удаляют худшие. Состоят из двух
 компонент:
 1. Меры значимости признаков µ;
 2. Правила обрезки κ, которое определяет, какие признаки необходимо удалить на
 основе µ.
 Классификация фильтрующихметодов
 1. Одномерные
 • Междудвумя вещественными числами:– Евклидово расстояние– Коэффициент корреляции (Пирсона или Спирмена)
 • Междувещественным числом и категорией:– Попарные расстояния (внутренние или внешние)– Условная дисперсия
 • Междудвумя категориями:– Прирост информации (IG)– Индекс Джини– χ2
 2. Многомерные
 • Выбор признаков на основе корреляций (CFS)
 • Фильтр марковского одеяла (MBF)
 Определение. Коэффициент корреляции Пирсона
 r =
 i,j
 (xij − xj)(yi − y)
 i,j
 (xij − xj)2 
i
 (yi − y)2
 ∈ [−1;1]
 Определение. Коэффициент корреляции Спирмена рассчитывается следующим
 алгоритмом:
 1. Отсортировать объекты двумя способами (по каждому из признаков);
 2. Найти ранги объектов для каждой сортировки;
 3. Вычислить корреляцию Пирсона между векторами рангов.
Лекция 10. Выбор признаков
 стр. 76 из 95
 В качестве правила обрезки κ можно брать:
 1. Число признаков;
 2. Порог значимости признаков;
 3. Интегральный порог значимости признаков;
 4. Метод сломанной трости;
 5. Метод локтя.
 Одномерные фильтры очень быстро работают и позволяют оценивать значимость каж
дого признака, однако они игнорируют отношения между признаками и то, что реально
 использует предсказательная модель.
 Многомерные фильтры работают существенно медленнее одномерных, но все еще го
раздо быстрее, чем методы-обертки. Их преимущество по сравнению с одномерными
 фильтрами состоит в том, что они учитывают отношения между признаками.
 46 Гибридыиансамбли
 Можнопопробовать использовать сильные стороны разных подходов, комбинируя их.
 Наиболее частый вариант:
 1. Применение фильтра или набора фильтров к исходному набору признаков с целью
 отсеивания лишних;
 2. Применение метода-обертки или встроенных методов к оставшимся признакам.
 Выделяются следующие способы комбинирования методов выбора признаков (см. рис.
 10.5, 10.6):
 1. Гибридный подход
 2. Ансамблирование при выборе признаков
 3. Ансамблирование на уровне моделей
 4. Ансамблирование на уровне ранжирования
 5. Ансамблирование на уровне мер значимости
 Применение гибридных и ансамблируюших методов чаще всего лучше по времени и ка
честву, однако интерпретируемость результата может снижаться. К тому же повышается
 риск переобучения.
Лекция 10. Выбор признаков
 стр. 77 из 95
 Рис. 10.5: а) Схема гибридного подхода, б) схема ансамблирования
 в выборе признаков, в) схема ансамблирования на уровне моделей
Лекция 10. Выбор признаков
 стр. 78 из 95
 Рис. 10.6: а) Схема ансамблирования на уровне ранжирования, б)
 схема ансамблирования на уровне мер значимости
Лекция 11. Извлечение признаков
 стр. 79 из 95
 Лекция 11
 Извлечение признаков
 Лектор: Алексей Сергеевич Забашта
 47 Методглавныхкомпонент
 Определение. Метод главных компонент (Principal Components Analysis, PCA) — один
 из основных способов уменьшить размерность данных, потеряв наименьшее количество
 информации.
 В одномерном случае дано множество точек в Rn и требуется описать эти данные при
 помощи одной переменной. Это задача регрессии без учителя. Идея решения — проеци
ровать на прямую, такую, что:
 1. расстояние от точек до нее минимально;
 2. дисперсия проекций максимальна.
 Примечание. Эти условия эквивалентны.
 В общемслучае стоит задача приблизить данные линейным многообразием меньшего
 размера, с требованиями к решению:
 • Минимизация расстояния;
 • Максимизация дисперсии проекций;
 • Максимизация расстояний между проекциями;
 • Корреляция между осями проекций равна нулю.
 Примечание. Существует много вариантов того, как можно описать требования к реше
нию.
Лекция 11. Извлечение признаков
 стр. 80 из 95
 Рис. 11.1: Иллюстрация одномерного случая
 Формальная постановка задачи такова. Имеется n числовых признаков fj(x), j = 1,...,n.
 Объекты обучающей выборки отождествляются с их признаковыми описаниями: x ≡
 (f1(xi), ..., fn(xi)), i = 1,...,l. Рассмотрим матрицу F, строки которой соответствуют
 признаковым описаниям обучающих объектов:
 
 Fl×n =
 
 
 f1(x1) ... fn(x1)
 ...
 ...
 ...
 f1(xl) ... fn(xl)
 =
 
 
 
 .
 x1
 ...
 xl
 Обозначим zi = (g1(xi),...,gm(xi)) признаковые описания тех же объектов в новом
 пространстве Z = Rm меньшей размерности m < n:
 
 Gl×m =
 
 g1(x1) ... gm(x1)
 ...
 g1(xl) ... gm(xl)
 
 ...
 ...
 =
 
 
 z1
 ...
 zl
 
 .
 Потребуем, чтобы исходные признаковые описания можно было восстановить по новым
 описаниям с помощью некоторого линейного преобразования, определяемого матрицей
 U =(ujs)n×m:
 m
 fj(x) =
 s=1
 или в векторной записи x = zUT.
 gs(x)ujs, j = 1,...,n, x ∈ X,
Лекция 11. Извлечение признаков
 стр. 81 из 95
 Примечание. Восстановленное описание x не обязано в точности совпадать с исходным
 описанием x, но их отличие на объектах обучающей выборки должно быть как можно
 меньше при выбранной размерности m.
 Будем искать одновременно и матрицу новых признаковых описаний G, и матрицу ли
нейного преобразования U, при которых суммарная невязка ∆2(G,U) восстановленных
 описаний минимальна:
 l
 ∆2(G,U) =
 i=1
 l
 ||xi − xi||2 =
 i=1
 ||ziUT − xi||2 = ||GUT −F||2 → min
 G,U 
,
 где все нормы евклидовы. Будем предполагать, что матрицы G и U не вырождены:
 rankG = rankU = m. Иначе существовало бы представление ¯ G¯ UT = GUT с числом
 столбцов в матрице ¯ G, меньшим m. Поэтому интересны лишь случаи, когда m ≤ rankF.
 Теорема1. Еслиm ≤ rankF,томинимум∆2(G,U)достигается,когдастолбцыматрицы
 U есть собственные векторы FTF, соответствующие m максимальным собственным
 значениям. При этом G = FU, матрицы U иGортогональны.
 Примечание. Доказательство можно найти на вики-конспектах.
 Примечание. Подразумевается, что данные были нормализованы, поэтому FTF является
 матрицей корреляции.
 Следствия:
 1. UTU =Em—единичнаяматрица;
 2. GTG =Λ=diag(λ1,...,λm);
 3. UΛ =FTFU,GΛ=FTFG;
 4. ||GUT −G||2 = ||F||2 −trΛ = n
 i=m+1
 λi
 Таким образом, если отсортировать собственные значения FTF по убыванию: λ(1) ≥
 ... ≥ λ(n), то
 E(k) = ||GUT −F||2
 ||F||2
 = λ(k+1) +... + λ(n)
 λ(1) + ... + λ(n)
 характеризует долю информации, теряемую при проекции, поэтому значение k можно
 выбирать по E(k).
 Примечание. Главныекомпонентыможноискатьитеративно.Сначаланеобходимонайти
 прямуюc1расстояниедокоторойминимально.Затемнакаждойитерацииискатьпрямую
 ci, ортогональную cj i−1
 j=1
 , расстояние до которой минимально.
 Такимобразом, PCA—этолинейноепреобразование совсемидостоинствами инедостат
ками, оноработает быстро иширокораспространенодлясжатияданныхивизуализации.
Лекция 11. Извлечение признаков
 стр. 82 из 95
 Улучшить его можно, используя нелинейные методы (главные кривые и главные много
образия).
 Вариации и родственники PCA:
 • Анализ независимых компонент (ICA);
 • EMPCA;
 • Ядерный PCA;
 • Канонический корреляционный анализ (CCA).
 48 Автокодировщики
 Определение. Автокодировщик (autoencoder) — это глубокая нейронная сеть, способ
ная строить низкоразмерныепредставленияданныхзасчетнелинейнойтрансформации.
 Основная идея заключается в том чтобы заставить сеть предсказывать (восстанавливать)
 то, что подается ей на вход, ограничив возможность обучиться тривиальному преобразо
ванию.
 Существует два варианта ограничения преобразования:
 1. Структурный: между входными и выходными слоями должен быть слой меньшей
 размерности, так называемое бутылочное горлышко (bootleneck). Это недопол
ненный(undercomplete) автокодировщик.
 2. Регуляризационный:добавимрегуляризационнуюконстантуквыходамэтогослоя,
 уменьшающим его размерность. Это разреженный (sparce) автокодировщик.
 Определение. Кодировщик (encoder) — часть сети от входного слоя до бутылочного
 горлышка.
 Определение. Декодировщик (decoder) — часть сети от бутылочного горлышка до
 выходного слоя.
 Пусть c — кодировщик, d — декодировщик, L — некая регуляризация, τ — коэффициент
 регуляризации. Минимизировать будем не ||d(c(x)) − x||, а
 ||d(c(x)) − x|| + τ · L(c(x))
 Примечание. Стандартно можно взять L1 норму.
 Вариации автокодировщика:
 • Шумоподавляющий (denoising) автокодировщик;
 • Сжимающий(contractive) автокодировщик;
 • Вариационный (variational) автокодировщик (основан на совсем других принци
пах).
Лекция 11. Извлечение признаков
 стр. 83 из 95
 Рис. 11.2: Архитектура автоэнкодера
 49 t-SNE
 Определение. Стохастическое вложение соседей с t-распределением (t-distributed
 stochastic neighbor embedding, t-SNE) — нелинейный алгоритм улучшения размерности,
 который используется для визуализации и пытается сохранять метрические отношения
 между объектами:
 1. Определяется вероятность для точки "выбрать ближайшим соседом"другую точку
 в пространстве;
 2. Строятся такие распределения для высокоразмерных и низкоразмерных представ
лений.
 3. Минимизируется расстояние Кульбака-Лейблера между этими двумя распределе
ниями.
 Определение. Расстояние (дивергенция) Кульбака-Лейблера (KL divergence) — рас
стояние между двумя распределениями P и Q:
 +∞
 DKL(P||Q) =
 −∞
 p(x)log p(x)
 q(x) d(x),
 где p распределено согласно P, а q — согласно Q. Также называется относительной
 энтропией.
Лекция 11. Извлечение признаков
 стр. 84 из 95
 Примечание. Заметим, что t-SNE — это безмодельный метод, так как если мы добавим
 еще данных, то придется пересчитывать матрицы расстояний и заново решить задачу
 оптимизации. Однако можно сделать модельный t-SNE, если считать, что мы получаем
 объекты на выходе изобъектов на входе с помощьюкакой-топараметрической функции.
 Рис. 11.3: Общая схема t-SNE
 Распределения для обоих пространств определяется так:
 exp −||xi − xj||2
 pj|i =
 qj|i =
 2σ2
 k=j 
exp −||xi − xk||2
 2σ2
 exp(−||yi − yj||2)
 k=j 
exp(−||yi − yk||2)
 Вариант определения распределений:
 pij = pj|i + pi|j
 2|X|
 qj|i =
 exp(−||yi − yj||2)
 k=l 
exp(−||yk − yl||2)
Лекция 11. Извлечение признаков
 стр. 85 из 95
 Симметричное стохастическое вложение соседей с t-распределением — модификация
 для t-SNE, которая заменяет распределение на t-расрпеделение Стьюдента:
 qij = (1+||yi −yj||2)−1
 k=l
 (1 + ||yk − yl||2)−1
 Определение. Многомерное шкалирование (MDS) — модификация t-SNE, которая
 не строит матрицы вероятностей, а работает непосредственно с матрицами расстояний.
 Однако на практике такая модель показывает себя хуже.
 Рис. 11.4: Общая схема MDS
 50 Обучениенаодномпримере
 Постановка задачи: На КПП по лицам нейронная сеть определяет является ли человек
 сотрудником. Если решать такую проблему как задачу классификации, то для каждого
 нового сотрудника необходимо брать много фотографий и переобучать сеть, так как на
 каждого сотрудника выделяется отдельный класс. Однако это крайне неэффективно.
 Таким образом, основные проблемы данного сценария:
Лекция 11. Извлечение признаков
 стр. 86 из 95
 • Число классов может изменяться;
 • Внекоторых классах сравнительно мало объектов.
 Определение. Обучение на одном примере (one-short learning — это постановка, в
 которой алгоритм должен дообучиться классификации на новый класс, содержащий
 всего один объект.
 Определение. Обучение на нескольких примерах ((few-short learning)) предполагает
 всё то же, но с несколькими объектами.
 Обучение на одном примере основано на метрической классификации. Добавление цен
троида не заставляет менять другие центроиды. Однако необходимо, чтобы выученная
 метрика позволяла хорошо по ним классифицировать.
 Определение. Сиамская сеть (Siamese network) состоит из двух идентичных частей,
 возвращающих векторные представления входов.
 Для обучения сиамской сети используется triplet loss:
 L(a,p,n) = max(dist(a,p) − dist(a,n) + ε,0),
 где a– это anchor, p– объект того же класса, n- объект другого класса.
 Примечание. Еслипроанализироватьtriplet loss, становится понятно, что если сеть выдает
 похожие эмбеддинги для объектов одного класса, то dist(a,p) → 0, а если для ощутимо
 разные эмбеддингидляобъектовразныхклассов, тоdist(a,n) будет большим. При таком
 поведении итоговое значение ошибки будет нулевым или около нулевым.
 Таким образом, сеть обучается по батчам из троек градиентным спуском. Причем обуче
ние можно проводить на любых объектах необходимой природы. В примере с сотрудни
ками это могут быть произвольные фотографии людей.
 Для каждого класса хранится объект (центроид), с которым сравнивается вход по ко
синусному расстоянию. Чтобы добавить класс, добавляется новый объект в качестве
 центроида.
 51 Переносзнаний
 Мыужеобсуждали, что перенос знаний может ускорить обучение и качество решения
 задачи, однако можно использовать его для извлечения признаков. То есть, если взять
 фрагмент уже обученной сети, то можно считать эмбеддигами объектов то, что эта сеть
 выдает на скрытых слоях.
 52 Самообучение
 Определение. Самообучение(self-supervised learning) — подход, в которомдляобучения
 представлений используются задачи, в которых разметку можно получить автоматиче
Лекция 11. Извлечение признаков
 стр. 87 из 95
 ский.
 Определение. Предварительная задача (pretext task) — задача с искусственно создан
нымиметками (псевдо-метками), на которой обучается модель, чтобы выучить хорошие
 представления (representations) объектов.
 Определение. Последующая задача (downstream task) — целевая задача, для которой
 используются полученные представления с простым дискриминатором.
 Примеры самообучения:
 • Определение поворота. Для такой задачи мы поворачиваем картинки, например,
 на 0, 90, 180 и 270 градусов, получая таким образом набор данных с разметкой, а
 модели нужно предсказать, как повернут кадр;
 • Предсказание контекста. Например, можно вырезать с картинки патчи (квадрат
ные фрагменты), расположенные вокруг некоторого центрального патча. Задача
 модели — определить по двум патчам, положение второго относительно первого
центрального;
 • Решение головоломки. Набор данных формируется разрезанием квадратных изоб
ражений, например, на 9 патчей и перемешиванием их. Задача модели– восстано
вить исходную картинку (собрать пазлы);
 • Восстановление части изображения;
 • Восстановление цвета изображения;
 • Предсказание слова по контексту (continuous bag of words, CBOW);
 • Предсказание контекста по слову (skip-gram).
 Определение. Сравнительное обучение (contrastive learning) — обучение за счет объ
единения аугментированных или иным способом полученных трансформаций в один
 класс с исходным объектом в парадигме метрической классификации.
 Пример. Использованиекропов.Мыдолжнывыучитьвекторноепредставлениеобъектов,
 удовлетворяющеетому,чточастиобъектовболеепохожинаобъект,чемналюбойдругой
 объект.
 53 Векторныепредставления слов
 Как ужебылорассмотреноналекциипроанализтекста,существуютследующиеспособы
 представления текста:
 • One-hot encoding;
 • Word2Vec:строитсянаидее,чтословаможнохарактеризоватьконтекстом,вкотором
 они встречаются, и дистрибутивной гипотезе о том, что слова, встречающиеся в
Лекция 11. Извлечение признаков
 стр. 88 из 95
 похожем контексте вероятно, будут иметь похожий смысл;
 • BERTипорожденные иммодели;
 • ELMO;
 • FastText;
 • RusVect¯ or¯ es (для русского языка).
Лекция 12. Генеративные модели
 стр. 89 из 95
 Лекция 12
 Генеративные модели
 Лектор: Алексей Сергеевич Забашта
 54 Задачагенерации новыхобъектов
 Задачу генерации новых объектов принято относить к задаче обучения без учителя, так
 как нет никакого правильного варианта того, как именно генерируемые данных должны
 выглядеть.
 : При генерации стоит две задачи, между которыми необходимо искать баланс:
 • Мыхотимсоздатьобъекткоторыйправдоподобенвотношениинекоторойскрытой
 структуры объектов (то есть похож на существующие объекты);
 • Объект должен быть именно новым, а не дублем уже существующего объекта.
 По заданной выборке требуется генерировать новые образцы из того же распределения.
 Например, на вход подается набор изображений и на выходе должен быть другой набор
 похожих на исходные изображений.
 Примечание. Чтобы измерять сходство распределений можно использовать диверген
циюКульбака-Лейблера, о которой речь шла на прошлой лекции.
 Примечание. Важно не забывать, что дивергенция Кульбака-Лейблера несимметрична, а
 значит, не является расстоянием.
 55 PixelCNNиPixelRNN
 Примечание. Такие методы еще называются авторегрессионными.
 Определение. PixelRNN — это такая архитектура, которая выполняет генерацию изоб
ражения, начиная с левого верхнего угла. Зависимость от предыдущих пикселей моде
Лекция 12. Генеративные модели
 стр. 90 из 95
 Рис. 12.1: Таксономия генеративных моделей
 лируется с помощью RNN (LSTM или biLSTM, чтобы учитывать не только пиксель слева,
 но ипиксель сверху).
 Определение. PixelCNN — архитектура, аналогичная PixelRNN, также генерирующая
 изображения, начиная с левого верхнего угла. Таким образом, свертка по уже сгенериро
ванным пикселям генерирует новые. Её преимущество состоит в том, что она обучается
 быстрее, чем PixelRNN.
 Обе эти архитектуры обладают следующими преимуществами:
 • Онимогут явно вычислить правдоподобие p(x);
 • Явное правдоподобие обучающих данных дает хорошую метрику оценки;
 • Даютхорошие образцы.
 Основной их недостаток состоит в том, что они выполняют генерацию последовательно,
 а это медленно.
 Примечание. На сегодняшний день эти методы считаются устаревшими.
 56 Вариационныеавтокодировщики
 Определение. Вариационный автокодировщик (Variational Autoencoder, VAE) — ав
токодировщик (генеративная модель, которая учится отображать объекты в заданное
 скрытое пространство и обратно), основанный на вариационном выводе.
 При использовании обычного автокодировщика для генерации новых объектов (жела
тельно из того распределения, что и исходный набор данных) возникает следующая
 проблема. Неизвестны параметры распределения скрытого пространства, которые требу
ется установить, чтобы картинка, после применения декодера, стала похожа на картинки
 из исходного набора данных, но при этом не совпадала ни с одной из них. Обыкновен
Лекция 12. Генеративные модели
 стр. 91 из 95
 ный автокодировщик не может ничего утверждать про распределение скрытого вектора
 и даже про его область определения (например, она может быть дискретной).
 Вариационныйавтокодировщик,всвоюочередь,предоставляетвозможностьопределить
 распределение скрытого вектора за счет наложения ограничений на него. В частности,
 благодаря регуляризации можно заставить сеть генерировать скрытые вектор только из
 стандартного нормального распределения N(0,1).
 ВтакомвариантеунаспоявляетсявозможностьдаватьдекодерунавходвекторизN(0,1)
 иполучатьнавыходесгенерированныйобъект,похожийнаобъектыизисходногонабора
 данных.
 Рис. 12.2: Архитектура вариационного автокодировщика
 преимущества вариационного автокодировщика:
 1. Принципиальный подход к генеративным моделям;
 2. Позволяет сделать вывод q(z|x), может быть полезным представлением функции
 для других задач.
 Недостатки:
 1. Максимизация нижней границы вероятности — не такая хорошая оценка, как
 PixelRNN или PixelCNN.
 2. Образцы более размытые и более низкого качества по сравнению с современными
 (GAN).
Лекция 12. Генеративные модели
 стр. 92 из 95
 Рис. 12.3: Простая блок-схема GAN
 57 Генеративно-состязательные модели
 Определение. Порождающие состязательные сети (Generative Adversarial Nets, GAN)
 — алгоритм машинного обучения, входящий в семейство порождающих моделей и
 построенный на комбинации из двух нейронных сетей:
 1. Генератора — генеративной модели G, которая строит по вектору случайных
 шумов приближение распределения данных
 2. Дискриминатора — дискриминативной модели D, оценивающей вероятность,
 что образец пришел из тренировочных данных, а не сгенерированных моделью G.
 Обучение для модели G заключается в максимизации вероятности ошибки дискрмина
тора D.
 Минимаксная целевая функция выглядит следующим образом:
 min
 θg 
max
 θd 
[Ex∼pdata 
log Dθd
 (x) + Ez∼p(z) log1 − Dθd
 (Gθg
 (z)) ],
 где
 • Gθg 
—дискриминатор с параметрами θd,
 • Gθd 
—генератор с параметрами θg.
 Примечание. Дискриминатор пытается максимизировать целевую функцию и сделать
 так, чтобыD(x)былблизокк1(настоящий)иD(G(z))былблизокк0(сгенерированный).
 Генератор старается него обмануть и добиться, чтобы D(G(z)) стал близок к 1.
 Обучение происходит поочередно:
 1. Градиентный подъем по дискриминатору:
 max
 θd 
[Ex∼pdata 
log Dθd
 (x) + Ez∼p(z) log1 − Dθd
 (Gθg
 (z)) ];
Лекция 12. Генеративные модели
 стр. 93 из 95
 2. Градиентный спуск по генератору
 min
 θg 
Ez∼p(z) log1 − Dθd
 (Gθg
 (z)) .
 Примечание. Вместо второго правила иногда используют градиентный подъем на гене
раторе:
 max
 θg 
Ez∼p(z) logDθd
 (Gθg
 (z)) .
 Это позволяет быстрее двигаться в начале обучения, так как градиенты вблизи нуля у
 этой функции выше.
 Рис. 12.4: Шаг обучения дискриминатора
 Недостатками GAN’ов являются:
 • Схлопывание мод распределения (mode collapse) — ситуация при которой генера
тор всегда создает одинаковые объекты, с помощью которых у него лучше всего
 получается обманывать дискриминатор;
 • Осцилляция;
 • Нет индикатора, когда останавливаться.
Лекция 12. Генеративные модели
 стр. 94 из 95
 Рис. 12.5: Шаг обучения генератора
 58 ИнтересныеидеивGAN’ах
 Определение. Conditional Gans (CGANs) — улучшение GAN, которое позволяет до
бавить несколько меток, чтобы дискриминатор могу работать как классификатор по
 отношению к некоторым меткам. В таком случае у нас разное распределение для каждо
го класса, а целевая функция выглядит так:
 min
 θg 
max
 θd 
[Ex∼pdata 
log Dθd
 (x,y) + Ez∼p(z) log1 − Dθd
 (Gθg
 (x,y),y) ]
 Определение. Задача оптимального перемещения — идея улучшения GAN, которая
 заменяет KL на расстояние Вассерштайна:
 W(pdata,pgen) =
 inf
 γ∈ (pdata,pgen) 
E(x,y)∼γ[||x − y||],
 где (pdata,pgen) — это множество совместных распределений над pdata и pgen.
 Примечание. Его нельзя считать напрямую, но можно найти приблизительное решение,
 которое поможет бороться с mode collapse.
 Ещеодна идея, как можно достигать хорошего качества — усложнять генератор и дис
криминатор в процессе обучения.
Лекция 12. Генеративные модели
 стр. 95 из 95
 Рис. 12.6: Усложнение GAN в процессе обучения
 59 Диффузныемодели
 Примечание. Диффузные модели сейчас наиболее популярны (пример- Midjourney).
 Идея метода основана на диффузии. На стадии обучения к изображению итеративно
 добавляется небольшой шум q. Модель p обучается убирать этот шум. При этом можно
 добавить условие на то, что должно скрываться за шумом.